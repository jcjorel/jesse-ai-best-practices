<!-- ⚠️ DO NOT EDIT MANUALLY! DOCUMENT AUTOMATICALLY GENERATED! ⚠️ -->
<!-- This file is automatically generated by the JESSE Knowledge Base system. -->
<!-- Manual edits will be overwritten during the next generation cycle. -->
<!-- To modify content, update the source files and regenerate the knowledge base. -->
# Directory Knowledge Base {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/llm/

## Global Summary

#### Functional Intent & Features

This directory serves as the Large Language Model integration layer for the Jesse Framework MCP Server, providing comprehensive AI conversation capabilities through specialized driver implementations for advanced language models within the broader MCP ecosystem. The directory enables developers to integrate sophisticated AI conversation features including real-time streaming, intelligent caching, memory management, and conversation persistence through modular driver architectures designed for production deployment scenarios. Key semantic entities include `StrandsClaude4Driver` main driver class for AWS Bedrock Claude 4 Sonnet integration, `ConversationManager` for session and cache management, `Claude4SonnetConfig` configuration class with factory methods `create_optimized_for_conversations()`, `create_optimized_for_analysis()`, and `create_optimized_for_performance()`, `PromptCache` class for in-memory response caching with TTL and LRU eviction policies, `ConversationMemoryStrategy` enum with `SUMMARIZING`, `SLIDING_WINDOW`, and `NULL` memory management options, streaming response classes `StreamingResponse` and `ConversationResponse`, comprehensive exception hierarchy including `StrandsDriverError`, `ConversationError`, `ModelConfigurationError`, `CacheError`, `BedrockConnectionError`, `StreamingError`, and `TokenLimitError`, AWS integration through environment variables `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_PROFILE`, and `AWS_REGION`, Strands Agent SDK components `Agent`, `BedrockModel`, and `AgentResult`, official Claude 4 Sonnet model identifier `us.anthropic.claude-sonnet-4-20250514-v1:0`, and educational demonstration scripts with comprehensive documentation for developer onboarding and feature validation. The system implements async-first architecture with context manager patterns for resource lifecycle management, real streaming capabilities through external SDK integration, intelligent memory management based on token limits, and comprehensive error handling with graceful degradation for production reliability.

##### Main Components

The directory contains one primary subdirectory providing complete LLM integration capabilities for the Jesse Framework MCP Server ecosystem. The `strands_agent_driver/` subdirectory implements a comprehensive Claude 4 Sonnet driver package with six core files and educational resources including package initialization through `__init__.py` with 11 exported symbols, main driver implementation in `driver.py` with streaming and conversation management, session persistence and caching through `conversation.py`, configuration management and validation in `models.py`, hierarchical exception system in `exceptions.py`, comprehensive documentation in `README.md`, and educational demonstration scripts in `examples/` subdirectory showcasing basic conversations, streaming responses, prompt caching, reasoning suppression, and performance optimization patterns. The subdirectory provides complete AWS Bedrock integration through Strands Agent SDK with async context manager patterns, intelligent caching with SHA-256 hashing and TTL expiration, memory management strategies for token limit compliance, and comprehensive error handling with specific exception types for different failure scenarios.

###### Architecture & Design

The architecture implements a modular LLM integration layer with clear separation between driver implementations and the broader Jesse Framework MCP Server ecosystem, following plugin-based design patterns that enable extensibility for additional language model integrations. The design emphasizes async-first principles with comprehensive context manager support for resource lifecycle management, performance optimization through intelligent caching and memory management strategies, and developer experience through educational examples and comprehensive documentation. Key design patterns include the plugin architecture pattern enabling multiple LLM driver implementations, async context manager pattern for automatic resource management across all driver components, factory method pattern for optimized configuration presets tailored to different use cases, strategy pattern for memory management with pluggable algorithms, cache-aside pattern for prompt response caching with TTL and LRU eviction, adapter pattern for external SDK integration with graceful degradation, streaming iterator pattern for real-time response delivery, hierarchical exception pattern for precise error handling and debugging, and facade pattern for unified API access through package initialization. The system uses composition over inheritance with specialized classes for different concerns, dependency injection for configuration management, and comprehensive logging throughout all components for operational visibility and debugging support in production environments.

####### Implementation Approach

The implementation uses async-first architecture with comprehensive context manager patterns for proper resource lifecycle management across all LLM driver components and integration points. Driver implementations employ conditional imports with availability checking and mock class definitions for type safety when external dependencies are unavailable, enabling graceful degradation in development environments. Streaming implementation leverages real external SDK streaming capabilities through `stream_async()` methods with event processing for different stream types including data events, reasoning events, and tool usage events with metadata inspection. Memory management implements configurable strategies through pluggable algorithms including conversation summarization when token thresholds are reached, sliding window approaches keeping recent messages within limits, and null strategies for stateless interactions. Caching employs SHA-256 hashing for key generation combining prompt content, conversation identifiers, and configuration hashes with TTL-based expiration and LRU eviction policies for performance optimization. The approach implements comprehensive retry logic with exponential backoff for both regular and streaming operations, token estimation using character-to-token ratio approximation for performance optimization, and conversation persistence through in-memory dictionaries with efficient access patterns. Configuration management uses Python dataclasses with post-initialization validation for type-safe parameter handling and AWS environment variable integration for deployment flexibility.

######## External Dependencies & Integration Points

**→ Inbound:**
- `strands:Agent` (external library) - Strands Agent SDK main class for Claude 4 Sonnet interaction and real-time streaming capabilities
- `strands.models:BedrockModel` (external library) - Bedrock model wrapper for AWS integration and configuration management
- `strands.types:AgentResult` (external library) - response type for Strands Agent operations with optional availability checking
- `boto3` (external library) - AWS SDK for Python enabling Amazon Bedrock service access and authentication
- `asyncio` (external library) - async event loop management for driver operations, streaming processing, and context manager lifecycle
- `typing` (external library) - comprehensive type annotations for type safety across all LLM integration components
- `dataclasses` (external library) - structured configuration management with validation and factory methods
- `hashlib` (external library) - SHA-256 hashing for cache key generation and configuration fingerprinting
- `json` (external library) - configuration serialization for hash generation and data structure handling
- `time` (external library) - timestamp generation for cache TTL management and performance tracking
- `datetime` (external library) - ISO format timestamp generation for conversation tracking and message timestamping
- `logging` (external library) - structured logging for operations, error tracking, and debugging information
- `Amazon Bedrock` service - AWS managed AI service providing Claude 4 Sonnet model access with official inference profile ID
- AWS environment variables `AWS_REGION`, `AWS_PROFILE`, `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` - authentication and configuration

**← Outbound:**
- Jesse Framework MCP Server core components - consuming LLM integration layer for AI conversation capabilities and streaming functionality
- MCP resource handlers - using LLM drivers for intelligent resource processing and content generation
- Application integration layers - consuming driver APIs for conversation management, streaming functionality, and performance optimization
- Testing frameworks - using LLM integration components for functionality verification and regression testing
- Documentation generation systems - consuming package metadata and exports for API documentation
- Performance monitoring systems - using conversation statistics, cache performance metrics, and operational logs
- AWS Bedrock service integration - generating actual API calls, streaming responses, and cache operations for live interaction

**⚡ System role and ecosystem integration:**
- **System Role**: Core LLM integration layer within Jesse Framework MCP Server ecosystem, providing comprehensive AI conversation capabilities through modular driver implementations with streaming, caching, and memory management for production AI applications
- **Ecosystem Position**: Central integration component serving as the primary interface between Jesse MCP Server and external language model services, designed for modularity and extensibility while maintaining independence from core MCP server functionality
- **Integration Pattern**: Used by MCP server components through standard Python package imports and async context managers, consumed by resource handlers for intelligent processing, integrated with external AI services through specialized driver implementations, and supported by comprehensive documentation and educational examples for developer adoption

######### Edge Cases & Error Handling

The system handles missing external SDK dependencies through conditional imports with availability flags and mock class definitions for type checking when dependencies are unavailable, enabling development and testing without full external service access. AWS Bedrock connection failures are managed through specialized exception handling with region context and detailed troubleshooting guidance including credential validation, service access verification, and model availability confirmation. Configuration validation implements comprehensive constraint checking including temperature ranges, token limits, and logical consistency between related settings with specific error messages for debugging. Conversation management errors use domain-specific exceptions for session-specific issues with conversation ID context for debugging and automatic session creation fallbacks. Streaming failures employ retry logic with exponential backoff for transient network issues and incomplete chunk processing scenarios with graceful degradation to non-streaming modes. Cache operation failures are handled through try-catch blocks with warning-level logging to prevent cache errors from disrupting conversation flow while maintaining performance benefits when possible. Token limit violations trigger intelligent memory management strategies including conversation summarization and sliding window application to maintain context within model-specific token windows. The system provides comprehensive logging for all error conditions with appropriate log levels, detailed context information for debugging, and graceful degradation paths that maintain core functionality even when optional components fail.

########## Internal Implementation Details

The LLM integration layer uses lazy initialization patterns with thread-safe locks for external SDK component initialization including model wrappers and agent instances with configuration parameter injection. Cache implementation employs SHA-256 hash truncation to 16 characters for keys balancing uniqueness with storage efficiency, TTL-based expiration with periodic cleanup, and LRU eviction when size limits are exceeded. Memory management thresholds use configurable values including conversation summary thresholds (150000 tokens) for summarization triggers and maximum context tokens (180000 tokens) for sliding window limits with conversation summarization keeping recent messages while summarizing older content into system messages. Configuration dataclasses use specific field defaults including temperature (0.7), max_tokens (200000), streaming (True), enable_prompt_caching (True), cache_ttl_seconds (3600), max_retries (3), and exponential_backoff (True) with post-initialization validation and environment variable integration. Streaming implementation processes different event types including data events for text content, reasoning events for model thinking output with suppression control, and tool usage events with metadata inspection and filtering. The system maintains separate dictionaries for conversation contexts and message arrays enabling efficient access patterns, uses relative imports with dot notation for clean namespace organization, implements comprehensive statistics tracking including cache hit rates, token usage, conversation metrics, and performance measurements, and provides package-level API control through explicit symbol exports for clean integration boundaries.

########### Usage Examples

Basic LLM integration demonstrates the essential pattern for AI conversation setup with factory method configuration and async context management. This approach provides the foundational usage pattern for Jesse MCP Server components requiring AI conversation capabilities.

```python
# Initialize LLM driver with optimized configuration for MCP server integration
from jesse_framework_mcp.llm.strands_agent_driver import StrandsClaude4Driver, Claude4SonnetConfig

async def mcp_ai_integration():
    config = Claude4SonnetConfig.create_optimized_for_conversations(
        enable_prompt_caching=True,
        aws_region="us-west-2"
    )
    
    async with StrandsClaude4Driver(config) as driver:
        # Integrate with MCP resource processing
        await driver.start_conversation("mcp_session")
        response = await driver.send_message("Process this resource...", "mcp_session")
        
        return response.content
```

Advanced streaming integration showcases the comprehensive pattern for real-time AI response processing within MCP server workflows. This pattern demonstrates production-ready implementation with comprehensive error handling and performance monitoring for MCP resource handlers.

```python
# Advanced streaming AI integration for MCP server resource processing
from jesse_framework_mcp.llm.strands_agent_driver import (
    StrandsClaude4Driver, Claude4SonnetConfig,
    BedrockConnectionError, StreamingError
)

async def mcp_streaming_processor():
    config = Claude4SonnetConfig.create_optimized_for_analysis(streaming=True)
    
    try:
        async with StrandsClaude4Driver(config) as driver:
            await driver.start_conversation("mcp_analysis")
            
            # Stream AI analysis for MCP resource
            async for chunk in driver.stream_conversation("Analyze resource data...", "mcp_analysis"):
                if chunk.metadata.get("stream_event") == "data":
                    yield chunk.content
                elif chunk.is_complete:
                    break
                    
    except BedrockConnectionError as e:
        # Fallback to non-AI processing for MCP server resilience
        yield f"AI unavailable, using fallback processing: {e.region}"
```

## Subdirectory Knowledge Integration

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/llm/strands_agent_driver/
*Last Updated: 2025-07-05T12:27:41Z*

This directory implements a comprehensive Claude 4 Sonnet driver package for Amazon Bedrock integration using the Strands Agent SDK, providing a complete asyncio-based conversation system with streaming capabilities, intelligent caching, memory management, and educational examples within the Jesse Framework MCP Server ecosystem. The package enables developers to integrate advanced AI conversation features through pure asyncio interfaces with real-time streaming, prompt caching optimization, multiple memory management strategies, and complete independence from Jesse MCP Server components while offering clean integration pathways. Key semantic entities include `StrandsClaude4Driver` main driver class with async context manager support, `ConversationManager` for session and cache management, `Claude4SonnetConfig` configuration class with factory methods `create_optimized_for_conversations()`, `create_optimized_for_analysis()`, and `create_optimized_for_performance()`, `PromptCache` class for in-memory response caching with TTL and LRU eviction, `ConversationMemoryStrategy` enum with `SUMMARIZING`, `SLIDING_WINDOW`, and `NULL` options, `Claude4SonnetModel` enum with official model identifier `us.anthropic.claude-sonnet-4-20250514-v1:0`, core methods `start_conversation()`, `send_message()`, `stream_conversation()`, `get_conversation_stats()`, streaming response classes `StreamingResponse` and `ConversationResponse`, comprehensive exception hierarchy including `StrandsDriverError`, `ConversationError`, `ModelConfigurationError`, `CacheError`, `BedrockConnectionError`, `StreamingError`, and `TokenLimitError`, AWS environment variables `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_PROFILE`, and `AWS_REGION`, Strands Agent SDK components `Agent`, `BedrockModel`, and `AgentResult`, and educational demonstration scripts in `examples/` directory with comprehensive documentation and troubleshooting guidance. The system implements async-first architecture with context manager patterns, real streaming capabilities through Strands Agent SDK, intelligent memory management based on token limits, and comprehensive error handling with graceful degradation.

##### Main Components

The directory contains six primary files and one subdirectory providing complete Claude 4 Sonnet driver functionality and educational resources. The `__init__.py` module serves as the package initialization and public API definition with 11 exported symbols including main classes, configuration components, and exception hierarchy. The `driver.py` file implements the main `StrandsClaude4Driver` class with streaming capabilities, conversation management, and AWS Bedrock integration through Strands Agent SDK. The `conversation.py` module provides `ConversationManager` and `PromptCache` classes for session persistence, intelligent caching, and memory management strategies. The `models.py` file defines configuration classes including `Claude4SonnetConfig`, `ConversationContext`, and enums for model identifiers and memory strategies with comprehensive validation and factory methods. The `exceptions.py` module implements a hierarchical exception system with seven specialized exception classes for precise error handling and debugging support. The `README.md` documentation provides comprehensive guidance for installation, configuration, usage patterns, and integration with Jesse MCP Server applications. The `examples/` subdirectory contains five demonstration scripts and documentation showcasing basic conversations, streaming responses, prompt caching, reasoning suppression, and performance optimization with detailed educational content.

###### Architecture & Design

The architecture implements a layered design pattern with clear separation between driver logic, conversation management, configuration handling, and error management, following async-first principles with comprehensive context manager support for resource lifecycle management. The design emphasizes modularity through well-defined interfaces, performance optimization through intelligent caching and memory management, and developer experience through comprehensive documentation and educational examples. Key design patterns include the async context manager pattern for automatic resource management, factory method pattern for optimized configuration presets, strategy pattern for memory management with pluggable algorithms, cache-aside pattern for prompt response caching, adapter pattern for Strands SDK integration with graceful degradation, streaming iterator pattern for real-time response delivery, hierarchical exception pattern for precise error handling, and facade pattern for unified API access through package initialization. The system uses composition over inheritance with specialized classes for different concerns, dependency injection for configuration management, and comprehensive logging throughout all components for operational visibility and debugging support.

####### Implementation Approach

The implementation uses Python dataclasses with post-initialization validation for type-safe configuration management and async context managers with proper resource lifecycle handling across all components. Strands SDK integration employs conditional imports with availability checking and mock class definitions for type safety when dependencies are unavailable. Streaming implementation leverages real Strands Agent SDK streaming through `stream_async()` method with event processing for different stream types including data, reasoning, and tool usage events. Memory management implements three distinct strategies through configurable algorithms: conversation summarization when token thresholds are reached, sliding window approach keeping recent messages within limits, and null strategy for stateless interactions. Caching employs SHA-256 hashing for key generation combining prompt content, conversation ID, and configuration hash with TTL-based expiration and LRU eviction policies. The approach implements comprehensive retry logic with exponential backoff for both regular and streaming operations, token estimation using character-to-token ratio approximation, and conversation persistence through in-memory dictionaries with efficient access patterns. Error handling uses specific exception types with detailed context and graceful degradation for missing dependencies or configuration issues.

######## External Dependencies & Integration Points

**→ Inbound:**
- `strands:Agent` (external library) - Strands Agent SDK main class for Claude 4 Sonnet interaction and real-time streaming capabilities
- `strands.models:BedrockModel` (external library) - Bedrock model wrapper for AWS integration and configuration management
- `strands.types:AgentResult` (external library) - response type for Strands Agent operations with optional availability checking
- `boto3` (external library) - AWS SDK for Python enabling Amazon Bedrock service access and authentication
- `asyncio` (external library) - async event loop management for driver operations, streaming processing, and context manager lifecycle
- `typing` (external library) - comprehensive type annotations for type safety across all components
- `dataclasses` (external library) - structured configuration management with validation and factory methods
- `hashlib` (external library) - SHA-256 hashing for cache key generation and configuration fingerprinting
- `json` (external library) - configuration serialization for hash generation and data structure handling
- `time` (external library) - timestamp generation for cache TTL management and performance tracking
- `datetime` (external library) - ISO format timestamp generation for conversation tracking and message timestamping
- `logging` (external library) - structured logging for operations, error tracking, and debugging information
- `Amazon Bedrock` service - AWS managed AI service providing Claude 4 Sonnet model access with official inference profile ID
- AWS environment variables `AWS_REGION`, `AWS_PROFILE`, `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` - authentication and configuration

**← Outbound:**
- Jesse Framework MCP Server integration - consuming driver package through async context manager for Claude 4 Sonnet capabilities
- Application integration layers - using driver API for conversation management, streaming functionality, and performance optimization
- Example demonstration scripts - importing driver components for educational examples and feature validation
- Testing frameworks - consuming driver exports for functionality verification and regression testing
- Documentation generation systems - using package metadata and exports for API documentation
- Performance monitoring systems - consuming conversation statistics, cache performance metrics, and operational logs
- AWS Bedrock service integration - generating actual API calls, streaming responses, and cache operations for live interaction

**⚡ System role and ecosystem integration:**
- **System Role**: Complete Claude 4 Sonnet driver package within Jesse Framework MCP Server ecosystem, providing comprehensive AWS Bedrock integration with streaming, caching, conversation management, and memory optimization capabilities for production AI applications
- **Ecosystem Position**: Central driver component serving as the primary interface for Claude 4 Sonnet functionality, designed for independence from Jesse MCP Server while enabling seamless integration through well-defined APIs and comprehensive configuration management
- **Integration Pattern**: Used by applications through standard Python package imports and async context managers, consumed by Jesse MCP Server through direct driver instantiation, integrated with AWS Bedrock services through Strands Agent SDK adapter pattern, and supported by educational examples and comprehensive documentation for developer adoption

######### Edge Cases & Error Handling

The system handles missing Strands Agent SDK through conditional imports with `STRANDS_AVAILABLE` flag and mock class definitions for type checking when dependencies are unavailable. AWS Bedrock connection failures are managed through `BedrockConnectionError` with region context and detailed troubleshooting guidance including credential validation and service access verification. Configuration validation raises `ModelConfigurationError` for invalid parameters with comprehensive constraint checking including temperature ranges, token limits, and logical consistency between related settings. Conversation management errors use `ConversationError` for session-specific issues with conversation ID context for debugging and automatic session creation fallbacks. Streaming failures employ `StreamingError` with retry logic and exponential backoff for transient network issues and incomplete chunk processing scenarios. Cache operation failures are handled through try-catch blocks with warning-level logging to prevent cache errors from disrupting conversation flow. Token limit violations trigger intelligent memory management strategies including conversation summarization and sliding window application to maintain context within Claude 4 Sonnet's 200K token window. The system provides comprehensive logging for all error conditions with appropriate log levels, detailed context information for debugging, and graceful degradation paths that maintain functionality even when optional components fail.

########## Internal Implementation Details

The driver uses lazy initialization with `_is_initialized` flag and async lock for thread-safe initialization of Strands components including `BedrockModel` with configuration parameters and `Agent` with empty tools list for non-MCP usage. Cache implementation employs SHA-256 hash truncation to 16 characters for keys balancing uniqueness with storage efficiency, TTL-based expiration with periodic cleanup, and LRU eviction when size limits are exceeded. Memory management thresholds use configurable values including `conversation_summary_threshold` (150000 tokens) for summarization triggers and `max_context_tokens` (180000 tokens) for sliding window limits with conversation summarization keeping the last 3 messages while summarizing older content. Configuration dataclass uses specific field defaults including temperature (0.7), max_tokens (200000), streaming (True), enable_prompt_caching (True), cache_ttl_seconds (3600), max_retries (3), and exponential_backoff (True) with post-initialization validation and AWS environment variable integration. Streaming implementation processes different event types including data events for text content, reasoning events for Claude 4 thinking output with suppression control, and tool usage events with metadata inspection. The package maintains separate dictionaries for conversation contexts and message arrays enabling efficient access patterns, uses relative imports with dot notation for clean namespace organization, and implements comprehensive statistics tracking including cache hit rates, token usage, conversation metrics, and performance measurements.

########### Usage Examples

Basic driver integration demonstrates the essential pattern for Claude 4 Sonnet setup with factory method configuration and async context management. This approach provides the foundational usage pattern for production applications requiring AI conversation capabilities.

```python

## File Knowledge Integration

*No files processed*

---
*Generated: 2025-07-05T12:27:41Z*
*Source Directory: {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/llm*
*Total Files: 0*
*Total Subdirectories: 1*

# End of llm_kb.md