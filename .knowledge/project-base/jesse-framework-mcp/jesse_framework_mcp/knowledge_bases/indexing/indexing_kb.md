<!-- ⚠️ DO NOT EDIT MANUALLY! DOCUMENT AUTOMATICALLY GENERATED! ⚠️ -->
<!-- This file is automatically generated by the JESSE Knowledge Base system. -->
<!-- Manual edits will be overwritten during the next generation cycle. -->
<!-- To modify content, update the source files and regenerate the knowledge base. -->
# Directory Knowledge Base {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/

## Global Summary

#### Functional Intent & Features

Comprehensive hierarchical knowledge base indexing system implementing Plan-then-Execute architecture for the Jesse Framework MCP Server, providing automated content analysis, structured knowledge file generation, and intelligent caching for large-scale codebase documentation. The system orchestrates LLM-powered content summarization through `HierarchicalIndexer`, `KnowledgeBuilder`, and `RebuildDecisionEngine` components with bottom-up assembly strategies ensuring child completion before parent processing. Enables developers to maintain synchronized knowledge bases through change detection, selective rebuilds, and atomic task execution with comprehensive error handling and performance optimization. Key semantic entities include `HierarchicalIndexer` for workflow orchestration, `FileAnalysisCache` for performance optimization, `ExecutionEngine` for dependency-aware task execution, `PlanGenerator` for decision-to-task translation, `StrandsClaude4Driver` for LLM integration, `IndexingConfig` for configuration management, `DirectoryContext` and `FileContext` for structure representation, `DecisionReport` and `ExecutionPlan` for workflow coordination, `EnhancedPrompts` for specialized analysis templates, and `DebugHandler` for development workflow support using async-first architecture patterns with `FastMCP` context integration.

##### Main Components

Core orchestration components include `HierarchicalIndexer` class providing five-phase Plan-then-Execute workflow coordination, `KnowledgeBuilder` implementing LLM-powered content analysis with cache-first processing, and `ExecutionEngine` managing atomic task execution with dependency resolution. Decision-making infrastructure encompasses `RebuildDecisionEngine` for centralized rebuild logic, `PlanGenerator` for converting decisions into executable tasks, and `DecisionReport` structures for comprehensive audit trails. Content generation components feature `KnowledgeFileGenerator` for template-based markdown creation, `EnhancedPrompts` for specialized LLM analysis workflows, and `MarkdownParser` for AST-based document manipulation. Performance optimization includes `FileAnalysisCache` with timestamp-based staleness detection, upfront cache structure preparation, and metadata separation preventing contamination. Configuration management provides `IndexingConfigManager` with auto-generation capabilities, `defaults.py` with handler-specific templates, and specialized handlers `ProjectBaseHandler` and `GitCloneHandler` for scenario-specific processing. Development support includes `DebugHandler` for LLM interaction capture and replay, comprehensive error handling with graceful degradation, and structured logging throughout all components.

###### Architecture & Design

Implements Plan-then-Execute architecture separating decision-making from execution through distinct phases: Discovery builds complete `DirectoryContext` hierarchy, Decision Analysis generates comprehensive `DecisionReport` with change detection, Plan Generation converts decisions into atomic `ExecutionPlan` with dependencies, and Atomic Execution performs dependency-aware task execution. Uses leaf-first hierarchical processing ensuring proper dependency ordering where parent directories wait for all child directories and files through bottom-up assembly patterns. Employs centralized decision architecture consolidating scattered logic into `RebuildDecisionEngine` with file-first optimization preventing unnecessary directory rebuilds. Implements cache-first processing strategy through `FileAnalysisCache` integration with selective bypass mechanisms for rebuild scenarios. Uses template-based knowledge file generation replacing complex incremental updates with straightforward string template assembly and alphabetical sorting. Follows async-first design principles with `FastMCP` context integration enabling concurrent operations and real-time progress reporting throughout all processing phases.

####### Implementation Approach

Executes recursive directory discovery building complete hierarchy context through `_build_directory_context()` with configuration filtering via `should_process_file()` and `should_process_directory()` methods. Implements four-phase decision analysis workflow: individual file staleness checking, directory rebuild analysis, orphaned content detection, and selective cascading propagation up hierarchy. Uses decision-driven task generation matching `RebuildDecisionEngine` decisions to appropriate `TaskType` instances with comprehensive metadata embedding for independent execution. Employs continuation-based retry mechanisms maintaining conversation context across truncation recovery attempts providing significant token savings. Implements timestamp-based staleness detection through direct comparison without tolerance for consistent behavior across components. Uses AST-based markdown parsing through `mistletoe` for reliable document structure understanding and safe content editing. Provides comprehensive path mapping between source files and knowledge base structure using project-base directory mirroring with portable path handling for cross-platform compatibility.

######## External Dependencies & Integration Points

**→ Inbound:**
- `fastmcp.Context` - FastMCP framework context interface for async progress reporting and user interaction throughout indexing workflows
- `jesse_framework_mcp.llm.strands_agent_driver:StrandsClaude4Driver` - Claude 4 Sonnet LLM integration for content analysis and knowledge generation
- `jesse_framework_mcp.llm.strands_agent_driver:Claude4SonnetConfig` - LLM configuration optimization for analysis tasks and conversation management
- `jesse_framework_mcp.knowledge_bases.models:IndexingConfig` - Configuration management providing processing parameters and filtering rules
- `jesse_framework_mcp.knowledge_bases.models:DirectoryContext` - Directory structure representation with file and subdirectory contexts
- `jesse_framework_mcp.knowledge_bases.models:FileContext` - Individual file metadata including path, size, and modification timestamps
- `jesse_framework_mcp.knowledge_bases.models.rebuild_decisions` - Decision model classes for structured outcomes and comprehensive reasoning
- `jesse_framework_mcp.knowledge_bases.models.execution_plan` - Execution planning models for atomic task management and dependency resolution
- `jesse_framework_mcp.helpers.path_utils:get_portable_path` - Cross-platform path conversion utility for consistent path formatting
- `mistletoe` (external library) - AST-based markdown parsing and rendering for reliable document manipulation
- `asyncio` (external library) - Async programming patterns and concurrency control for performance optimization
- `pathlib.Path` (external library) - Cross-platform path operations and filesystem metadata access
- `datetime` (external library) - Timestamp comparison and decision timing calculations
- `json` (external library) - Configuration serialization and debug metadata persistence
- `pydantic` (external library) - Configuration validation and type safety for indexing parameters

**← Outbound:**
- `jesse_framework_mcp/knowledge_bases/handlers/` - Knowledge base handlers consuming indexing services for different content types
- `{PROJECT_ROOT}/.knowledge/project-base/` - Generated knowledge base files with hierarchical structure mirroring
- `{PROJECT_ROOT}/.knowledge/git-clones/` - Specialized knowledge structures for git repository processing
- `debug_output/llm_debug/` - Debug interaction capture files for development workflow support
- `cache_files/*.analysis.md` - Cached analysis files for performance optimization and staleness detection
- MCP server implementations - Hierarchical processing coordination for knowledge base maintenance workflows
- Monitoring systems - Performance statistics and decision audit trails for system reliability tracking

**⚡ System role and ecosystem integration:**
- **System Role**: Core knowledge base indexing infrastructure for the Jesse Framework MCP Server, serving as the central orchestrator for automated content analysis, structured documentation generation, and intelligent caching across large-scale codebases
- **Ecosystem Position**: Central component bridging file system analysis with LLM processing capabilities, coordinating between specialized handlers, decision systems, and execution frameworks while maintaining comprehensive audit trails and performance optimization
- **Integration Pattern**: Used by MCP server handlers requiring hierarchical knowledge base maintenance, integrating with specialized scenario handlers for git-clones and project-base processing, coordinating with LLM-powered content generation through structured Plan-then-Execute workflows, and providing comprehensive debugging and monitoring capabilities for development and production environments

######### Edge Cases & Error Handling

Implements comprehensive truncation detection through `TruncationDetectedError` preventing any artifact creation when LLM responses are incomplete, ensuring knowledge base integrity through fail-fast approaches. Handles empty directories by detecting contentless directories and skipping knowledge file generation to prevent infinite rebuild loops through `_is_directory_empty_of_processable_content()`. Manages filesystem access restrictions with `OSError` and `PermissionError` catching enabling continued processing when individual items are inaccessible. Provides graceful degradation for cache failures ensuring cache operations never break core knowledge building processes through conservative fallback decisions. Implements bounded loop reviewer workflows preventing infinite loops through maximum iteration limits while preserving LLM work through forced caching. Handles missing dependencies by logging warnings and skipping task creation when `DecisionReport` lacks decisions for specific files or directories. Uses consolidated error handling patterns through `_handle_decision_error()` creating standardized error decisions with detailed context information for debugging and recovery. Manages circular dependency prevention by avoiding sibling dependencies in directory task generation using parent-child relationships exclusively.

########## Internal Implementation Details

Maintains performance tracking through `_decisions_made`, `_filesystem_operations`, and `_decision_start_time` counters for optimization monitoring across all indexing phases. Uses cached `_project_base_root` path for performance optimization in repeated path calculations throughout decision-making processes. Implements consolidated helper methods eliminating DRY violations including timestamp retrieval, path calculation, and decision creation patterns. Employs direct timestamp comparison without tolerance aligning with `FileAnalysisCache` behavior for consistent staleness detection. Uses `asyncio.Semaphore` for concurrency control limiting parallel operations to `config.max_concurrent_operations` with proper resource management. Maintains execution state through three sets tracking completed, failed, and running tasks for comprehensive workflow monitoring. Implements cache files with HTML comment metadata blocks using `CACHE_VERSION = "1.0"` for future compatibility and migration support. Uses hash-based interaction identification combining prompt content hashes with timestamps for unique interaction IDs enabling efficient debug replay functionality. Employs path normalization algorithms converting filesystem paths into underscore-separated filename components for cross-platform debug file compatibility.

########### Usage Examples

Essential hierarchical indexing initialization and execution workflow demonstrates the complete Plan-then-Execute architecture. This pattern shows how to coordinate all indexing components for comprehensive knowledge base generation with progress reporting and error handling.

```python
# Initialize complete indexing workflow with Plan-then-Execute architecture
from jesse_framework_mcp.knowledge_bases.indexing import HierarchicalIndexer
from jesse_framework_mcp.knowledge_bases.models import IndexingConfig

config = IndexingConfig(handler_type="project-base", debug_mode=True)
indexer = HierarchicalIndexer(config)

# Execute five-phase indexing workflow with comprehensive progress reporting
async def run_complete_indexing(source_root, ctx):
    status = await indexer.index_hierarchy(source_root, ctx)
    print(f"Processing: {status.processing_stats.progress_percentage:.1f}%")
    print(f"LLM calls: {status.processing_stats.llm_calls_made}")
    return status
```

Cache-first processing with debug capture demonstrates performance optimization and development workflow support. This approach shows how to leverage caching for efficiency while maintaining comprehensive debugging capabilities.

```python
# Cache-first processing with debug capture for development workflows
from jesse_framework_mcp.knowledge_bases.indexing.knowledge_builder import KnowledgeBuilder
from jesse_framework_mcp.knowledge_bases.indexing.debug_handler import DebugHandler

builder = KnowledgeBuilder(config)
debug_handler = DebugHandler(debug_enabled=True, enable_replay=True)

# Process with cache-first strategy and debug capture
async def process_with_caching_and_debug(file_context, ctx):
    # Check for cached analysis first
    cached_analysis = await builder.analysis_cache.get_cached_analysis(
        file_context.file_path, source_root
    )
    
    if cached_analysis:
        return cached_analysis
    
    # Perform LLM analysis with debug capture
    analysis = await builder.build_file_knowledge(file_context, ctx)
    debug_handler.capture_stage_llm_output("file_analysis", prompt, analysis, file_context.file_path)
    return analysis
```

Decision-driven execution planning showcases the Plan-then-Execute architecture with atomic task generation. This pattern demonstrates how high-level decisions are converted into executable tasks with comprehensive dependency management.

```python
# Decision-driven execution planning with atomic task generation
from jesse_framework_mcp.knowledge_bases.indexing.rebuild_decision_engine import RebuildDecisionEngine
from jesse_framework_mcp.knowledge_bases.indexing.plan_generator import PlanGenerator
from jesse_framework_mcp.knowledge_bases.indexing.execution_engine import ExecutionEngine

# Generate comprehensive decision report and execution plan
decision_engine = RebuildDecisionEngine(config)
plan_generator = PlanGenerator(config)
execution_engine = ExecutionEngine(config)

async def execute_plan_then_execute_workflow(root_context, source_root, ctx):
    # Phase 1: Generate comprehensive decision report
    decision_report = await decision_engine.analyze_hierarchy(root_context, source_root, ctx)
    
    # Phase 2: Convert decisions to atomic execution plan
    execution_plan = await plan_generator.create_execution_plan(
        root_context, decision_report, source_root, ctx
    )
    
    # Phase 3: Execute plan with dependency resolution
    results = await execution_engine.execute_plan(execution_plan, ctx)
    print(f"Completed: {len(results.completed_tasks)}, Failed: {len(results.failed_tasks)}")
    return results
```

## Subdirectory Knowledge Integration

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/image/
*Last Updated: 2025-07-06T23:05:36Z*

The `image/` directory represents a placeholder or reserved namespace within the Jesse Framework MCP knowledge base indexing system, designed to accommodate future image processing and visual content analysis capabilities. This directory exists as part of the hierarchical indexing architecture but currently contains no implementation, indicating either planned functionality for image-based knowledge extraction or a structural placeholder for visual content handling within the broader knowledge management ecosystem. The empty state suggests this component is either under development, deprecated, or represents a future extension point for multimedia content processing within the `HierarchicalIndexer` system. Key semantic entities that would be expected in this context include image processing handlers, visual content analyzers, OCR integration points, metadata extraction utilities, and multimedia knowledge base generation components, though none are currently present in the directory structure.

## File Knowledge Integration

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/__init__.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

This file serves as the package initialization module for the Knowledge Bases Hierarchical Indexing System, providing centralized component exports and clean dependency management for hierarchical knowledge base maintenance. The module exports core indexing components including `HierarchicalIndexer` for orchestration, `KnowledgeBuilder` for LLM-powered content summarization, and specialized handlers `GitCloneHandler` and `ProjectBaseHandler` for scenario-specific processing. Key semantic entities include the `__all__` export list defining the public API surface, async-first architecture patterns supporting concurrent processing operations, and bottom-up hierarchical processing without parent-to-child context dependencies. The system integrates with `strands_agent_driver` for LLM operations and follows `FastMCP Context` patterns for all async operations, enabling automated content summarization and knowledge base maintenance workflows.

##### Main Components

The file contains four primary exported components accessible through the package interface. The `HierarchicalIndexer` serves as the core indexing orchestrator managing the overall workflow. The `KnowledgeBuilder` provides LLM-powered content summarization capabilities for generating structured knowledge files. The `GitCloneHandler` and `ProjectBaseHandler` classes offer specialized processing for git-clone and project-base scenarios respectively. The module structure includes import statements from three internal modules: `hierarchical_indexer`, `knowledge_builder`, and `special_handlers`, with the `__all__` list explicitly defining the public API surface for clean dependency management.

###### Architecture & Design

The architecture implements a centralized export pattern with clear separation between orchestration, detection, and building components to prevent circular dependencies. The design follows async-first principles where all exported components support concurrent processing operations through `FastMCP Context` patterns. The package structure maintains clean separation of concerns with orchestration handled by `HierarchicalIndexer`, content generation by `KnowledgeBuilder`, and specialized scenarios by dedicated handler classes. The bottom-up hierarchical processing design ensures no parent-to-child context dependencies, enabling independent processing of directory hierarchies. The export pattern uses explicit `__all__` declaration providing controlled public API surface and preventing accidental exposure of internal implementation details.

####### Implementation Approach

The implementation uses selective component imports from three internal modules with explicit public API definition through the `__all__` list. The import strategy brings in the main orchestrator class, the content building class, and two specialized handler classes for different processing scenarios. The module follows Python package initialization conventions with docstring documentation explaining the package purpose and component relationships. The approach ensures all exported components integrate with the broader Jesse Framework MCP system through consistent async patterns and LLM integration points. The implementation maintains strict adherence to `JESSE_CODE_COMMENTS.md` standards for all exported components, ensuring consistent documentation and maintenance patterns across the indexing subsystem.

######## External Dependencies & Integration Points

**→ Inbound:**
- `.hierarchical_indexer.HierarchicalIndexer` - Core indexing orchestrator for workflow management
- `.knowledge_builder.KnowledgeBuilder` - LLM-powered content summarization component
- `.special_handlers.GitCloneHandler` - Git-clone scenario specialized processing
- `.special_handlers.ProjectBaseHandler` - Project-base scenario specialized processing

**← Outbound:**
- `../handlers/project_base_handler.py` - Consumes `ProjectBaseHandler` for project-base knowledge indexing
- `../handlers/git_clone_handler.py` - Consumes `GitCloneHandler` for git repository knowledge indexing
- `../main.py` - Primary entry point importing indexing components for MCP server operations
- `external_consumers/` - External systems importing indexing components for knowledge base automation

**⚡ System role and ecosystem integration:**
- **System Role**: Package initialization gateway for the Knowledge Bases Hierarchical Indexing System, serving as the primary import interface for all indexing-related components within the Jesse Framework MCP architecture
- **Ecosystem Position**: Central package interface providing controlled access to core indexing functionality, orchestrating the integration between LLM-powered content generation and specialized scenario handling
- **Integration Pattern**: Used by MCP server handlers for knowledge base operations, consumed by external automation systems requiring hierarchical indexing capabilities, and integrated with FastMCP Context patterns for async processing workflows

######### Edge Cases & Error Handling

The module handles import failures gracefully through Python's standard import mechanism, where missing dependencies would raise `ImportError` exceptions at package initialization time. Circular dependency prevention is managed through the clear separation of component responsibilities and the explicit import structure avoiding cross-references between exported classes. The `__all__` list prevents accidental exposure of internal implementation details that could lead to dependency issues in consuming code. Component integration errors are handled at the individual class level rather than at the package initialization level, ensuring that import failures provide clear error messages about missing dependencies. The async-first architecture requires proper error handling in all exported components to prevent unhandled promise rejections in concurrent processing scenarios.

########## Internal Implementation Details

The package uses standard Python `__init__.py` conventions with explicit imports from three internal modules following the pattern `from .module_name import ClassName`. The `__all__` list maintains exactly four exported components ensuring controlled public API surface and preventing internal implementation leakage. The module header includes comprehensive GenAI tool directives with change history tracking and design principle documentation following the established Jesse Framework patterns. The import structure avoids wildcard imports (`from module import *`) in favor of explicit class imports for better dependency tracking and IDE support. The docstring follows standard Python documentation conventions explaining the package purpose and component relationships for both human developers and automated documentation generation tools.

########### Code Usage Examples

**Basic package import for hierarchical indexing operations:** This example demonstrates importing the core indexing components for setting up knowledge base processing workflows with proper component separation.

```python
from jesse_framework_mcp.knowledge_bases.indexing import (
    HierarchicalIndexer,
    KnowledgeBuilder,
    GitCloneHandler,
    ProjectBaseHandler
)

# Initialize components for knowledge base processing
indexer = HierarchicalIndexer(config)
builder = KnowledgeBuilder(config)
```

**Specialized handler usage for different scenarios:** This example shows how to use the specialized handlers for git-clone and project-base scenarios with proper async context management.

```python
from jesse_framework_mcp.knowledge_bases.indexing import GitCloneHandler, ProjectBaseHandler

# Use specialized handlers based on processing scenario
if is_git_clone_scenario:
    handler = GitCloneHandler(config)
else:
    handler = ProjectBaseHandler(config)

await handler.process_directory(source_path, ctx)
```

**Complete indexing workflow integration:** This example demonstrates integrating all exported components for a complete knowledge base indexing workflow with proper error handling and async patterns.

```python
from jesse_framework_mcp.knowledge_bases.indexing import (
    HierarchicalIndexer,
    KnowledgeBuilder
)

# Complete indexing workflow
async def process_knowledge_base(source_root, config, ctx):
    indexer = HierarchicalIndexer(config)
    builder = KnowledgeBuilder(config)
    
    await indexer.process_hierarchy(source_root, builder, ctx)
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/config_manager.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

The `config_manager.py` file serves as the central configuration management system for the Jesse Framework MCP's knowledge base indexing operations, providing auto-generation of missing JSON configuration files from centralized Python defaults and comprehensive `Pydantic` validation for type-safe configuration management. This module enables user-customizable JSON configuration files while maintaining immutable configuration loading that prevents runtime modifications, evidenced by the `IndexingConfigManager` class with its `load_config()` method and configuration caching via `_config_cache`. Key semantic entities include `HandlerType` enum with values `PROJECT_BASE`, `GIT_CLONES`, and `PDF_KNOWLEDGE`, hierarchical `Pydantic` models like `FileProcessingConfigModel`, `ContentFilteringConfigModel`, and `LLMConfigModel`, plus integration with `.defaults` module through `get_default_config()` and `validate_handler_type()` functions. The system bridges centralized Python defaults with runtime JSON configurations through the `IndexingConfigModel` class that validates handler-specific parameters and converts to `IndexingConfig` instances via `from_dict()` method.

##### Main Components

The file contains the `IndexingConfigManager` class as the primary configuration orchestrator, seven specialized `Pydantic` validation models (`FileProcessingConfigModel`, `ContentFilteringConfigModel`, `LLMConfigModel`, `ChangeDetectionConfigModel`, `ErrorHandlingConfigModel`, `OutputConfigModel`, `DebugConfigModel`), the comprehensive `IndexingConfigModel` that aggregates all validation models, and the `HandlerType` enum defining supported indexing handler types. The manager provides core methods including `load_config()` for configuration loading with auto-generation, `_generate_default_config()` for JSON file creation, `_load_json_config()` for validation and loading, and `_convert_to_indexing_config()` for model transformation.

###### Architecture & Design

The architecture follows a hierarchical validation pattern with nested `Pydantic` models that mirror the configuration structure, enabling comprehensive type safety and validation at each level. The design implements an auto-generation strategy where missing JSON configuration files are created from centralized defaults, combined with immutable configuration loading that prevents runtime modifications through caching and conversion to `IndexingConfig` instances. The system uses a bridge pattern between Python defaults and JSON runtime configurations, with the `IndexingConfigManager` serving as the facade that coordinates between the defaults module, file system operations, and validation models.

####### Implementation Approach

The implementation employs `Pydantic` model validation with custom field validators like `validate_indexing_mode()` and `validate_project_base_exclusions()` that enforce handler-specific constraints and cross-field validation rules. Configuration loading uses a multi-stage approach: handler type validation, cache checking, auto-generation of missing files, JSON loading with UTF-8 encoding, `Pydantic` validation, and conversion to `IndexingConfig` instances. The system implements configuration caching through the `_config_cache` dictionary to optimize repeated loads, and uses atomic file operations with proper error handling for configuration generation and loading operations.

######## External Dependencies & Integration Points

**→ Inbound:** [configuration management dependencies]
- `.defaults:get_default_config` - centralized configuration template retrieval
- `.defaults:validate_handler_type` - handler type validation logic
- `..models.indexing_config:IndexingConfig` - target configuration class for conversion
- `json` (external library) - JSON serialization and parsing operations
- `pathlib` (external library) - cross-platform file system path operations
- `pydantic` (external library) - configuration validation and type safety

**← Outbound:** [configuration consumers]
- `knowledge_bases/indexing/` - indexing operations consuming validated configurations
- `*.indexing-config.json` - generated JSON configuration files for user customization
- `IndexingConfig` instances - validated configuration objects for indexing workflows

**⚡ System role and ecosystem integration:**
- **System Role**: Central configuration hub that bridges Python defaults with JSON runtime configurations for the knowledge base indexing system, ensuring type-safe configuration management across all indexing operations
- **Ecosystem Position**: Core infrastructure component that enables zero-configuration startup while supporting user customization through JSON files
- **Integration Pattern**: Used by indexing handlers during initialization to load validated configurations, with auto-generation ensuring seamless deployment and user customization through standard JSON editing workflows

######### Edge Cases & Error Handling

The system handles JSON parsing errors through `JSONDecodeError` catching with descriptive error messages, `Pydantic` validation failures with detailed field-level error reporting via `ValidationError.errors()`, and file system permission errors during configuration generation and loading. Handler type validation prevents unsupported configuration loading with comprehensive error messages listing supported types from `get_supported_handler_types()`. The manager handles missing configuration files through automatic generation, directory creation failures with graceful error propagation, and provides configuration cache clearing via `clear_cache()` for dynamic configuration updates.

########## Internal Implementation Details

The `_config_cache` dictionary provides performance optimization for repeated configuration loads, keyed by handler type strings. Configuration file naming follows the pattern `{handler_type}.indexing-config.json` within the knowledge directory structure. The `_convert_to_indexing_config()` method handles hierarchical to flat dictionary conversion using `model_dump()` and delegates to `IndexingConfig.from_dict()` for final object creation. File operations use UTF-8 encoding with proper exception handling, and JSON generation includes indentation and ASCII escaping configuration for human-readable output. The validation system supports cross-field validation through `field_validator` decorators with access to other field values via the `info` parameter.

########### Code Usage Examples

This example demonstrates initializing the configuration manager and loading a validated configuration for project-base indexing operations:

```python
from pathlib import Path
from jesse_framework_mcp.knowledge_bases.indexing.config_manager import IndexingConfigManager

# Initialize configuration manager with knowledge directory
config_manager = IndexingConfigManager(Path("./knowledge_bases"))

# Load configuration for project-base handler (auto-generates if missing)
config = config_manager.load_config("project-base")

# Access validated configuration parameters
print(f"Max file size: {config.max_file_size}")
print(f"Batch size: {config.batch_size}")
print(f"LLM model: {config.llm_model}")
```

This example shows how to clear the configuration cache and reload updated configurations:

```python
# Clear cache to force reload of updated configurations
config_manager.clear_cache()

# Get configuration file path for manual editing
config_path = config_manager.get_config_file_path("pdf-knowledge")
print(f"Edit configuration at: {config_path}")

# Reload configuration after manual changes
updated_config = config_manager.load_config("pdf-knowledge")
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/debug_handler.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

This module implements a comprehensive debug handler for LLM output persistence and replay in the Jesse Framework MCP knowledge base system, providing complete capture and reuse of LLM interactions for debugging markdown formatting issues and template generation problems. It provides extensive debugging functionality including stage-based LLM output capture, predictable filename generation for deterministic replay, memory-only operation for performance optimization, and comprehensive interaction metadata preservation. The system enables efficient debugging workflows by maintaining structured debug artifacts with pipeline stage organization while supporting both capture mode for saving LLM outputs and replay mode for loading saved outputs without redundant API calls. Key semantic entities include `DebugHandler`, `LLMInteraction`, `PIPELINE_STAGES`, `capture_stage_llm_output`, `get_stage_replay_response`, `capture_llm_interaction`, `get_replay_response`, `_normalize_path_for_filename`, `json`, `hashlib`, `pathlib.Path`, and stage-based directory organization. The implementation uses predictable filename generation based on normalized paths and pipeline stages enabling deterministic debug file locations for reliable debugging workflows.

##### Main Components

The module contains the primary `DebugHandler` class with core debugging operations including stage-based capture (`capture_stage_llm_output`), replay functionality (`get_stage_replay_response`), and complete interaction capture (`capture_llm_interaction`, `get_replay_response`). Supporting data structures include `LLMInteraction` dataclass for structured interaction storage with comprehensive metadata fields. Utility methods include path normalization (`_normalize_path_for_filename`), debug directory management (`_ensure_debug_directory`, `_create_stage_documentation`), and index management (`_update_debug_index`, `load_existing_interactions`). Administrative functions include debug summary generation (`get_debug_summary`) and cleanup operations (`cleanup`) for proper resource management and debugging session finalization.

###### Architecture & Design

The architecture follows a stage-based debug organization pattern with the `DebugHandler` class serving as the central coordinator for LLM interaction capture and replay using pipeline stage definitions (`PIPELINE_STAGES`) for clear debugging workflow understanding. The design implements dual-mode operation with memory-only caching for performance optimization when debug mode is disabled and persistent file storage for comprehensive debugging when enabled. Predictable filename generation uses normalized path components and stage identifiers enabling deterministic debug file locations for reliable replay functionality. The system uses structured data organization with separate files for prompts, responses, and metadata supporting easy manual inspection and modification. Error handling ensures debug operations never interfere with core knowledge building processes through graceful degradation and comprehensive exception management.

####### Implementation Approach

The implementation uses hash-based interaction identification combining prompt content hashes with timestamps for unique interaction IDs enabling efficient lookup and replay functionality. Path normalization employs character replacement and cleanup algorithms converting filesystem paths into underscore-separated filename components for cross-platform compatibility. Stage-based file organization creates separate subdirectories for each pipeline stage with predictable naming patterns enabling easy navigation and manual debugging workflows. Memory caching provides performance optimization through in-memory storage for non-debug mode operation while maintaining full functionality. The system employs lazy initialization minimizing overhead when debug mode is disabled and comprehensive metadata preservation ensuring complete context reproduction for debugging sessions.

######## External Dependencies & Integration Points

**→ Inbound:**
- `json` (standard library) - debug metadata serialization and structured data persistence
- `hashlib` (standard library) - content hashing for duplicate detection and unique interaction identification
- `pathlib.Path` (standard library) - debug file organization and cross-platform path handling
- `datetime` (standard library) - timestamp generation for debug artifact organization and chronological sorting
- `typing.Dict` (standard library) - type annotations for debug data structures and metadata parameters
- `typing.Optional` (standard library) - optional parameter handling for flexible debug capture
- `dataclasses` (standard library) - structured data containers for LLM interaction representation

**← Outbound:**
- `knowledge_builder.py:KnowledgeBuilder` - consumes debug handler for LLM interaction capture and replay
- `hierarchical_indexer.py:HierarchicalIndexer` - uses debug functionality for debugging indexing workflows
- `llm_debug/` directory structure populated with stage-organized debug artifacts
- Debug index files and documentation consumed by manual debugging workflows and analysis tools

**⚡ System role and ecosystem integration:**
- **System Role**: Auxiliary debugging component for the Jesse Framework MCP knowledge base system, providing comprehensive LLM interaction capture and replay capabilities for debugging markdown formatting and template generation issues
- **Ecosystem Position**: Peripheral debugging support component serving as the primary debugging infrastructure, integrating with knowledge builders and indexers for comprehensive debugging workflow support
- **Integration Pattern**: Used by knowledge building components for debug capture and replay, consumed by developers for manual debugging workflows while producing structured debug artifacts for analysis and troubleshooting

######### Edge Cases & Error Handling

The system handles debug directory creation failures by disabling debug functionality and falling back to memory-only operation, ensuring core processing continues when filesystem issues occur. Missing or corrupted debug files are handled gracefully during replay operations by returning `None` and allowing fallback to live LLM calls for continued processing. Path normalization errors are managed through fallback to generic filenames while logging warnings for debugging purposes. Interaction loading failures during replay initialization are handled individually with error logging while continuing to load remaining valid interactions. The system provides comprehensive error handling for JSON serialization failures, file I/O errors, and metadata corruption while maintaining debug session integrity and preventing debug failures from impacting main knowledge building operations.

########## Internal Implementation Details

The `PIPELINE_STAGES` dictionary defines five distinct processing stages (`stage_1_file_analysis`, `stage_2_chunk_analysis`, `stage_3_chunk_aggregation`, `stage_4_directory_analysis`, `stage_5_global_summary`) with descriptive purposes for clear debugging workflow organization. Path normalization uses character replacement algorithms converting path separators to underscores and handling special characters for filesystem-safe filename generation. Memory cache uses string-based keys combining stage names and normalized paths for efficient lookup during non-debug mode operation. Interaction ID generation combines processing type, prompt hash (8-character MD5), and ISO timestamp for unique identification enabling efficient replay lookup. The `_create_stage_documentation` method generates comprehensive README files with pipeline stage explanations, filename patterns, and debugging workflow instructions for user guidance.

########### Code Usage Examples

Basic debug handler initialization demonstrates the core setup pattern for enabling comprehensive LLM interaction capture with stage-based organization. This approach provides structured debugging capabilities with predictable file locations for reliable debugging workflows.

```python
# Initialize debug handler with stage-based organization
debug_handler = DebugHandler(
    debug_enabled=True,
    debug_output_directory=Path(".knowledge"),
    enable_replay=True
)

# Load existing interactions for replay functionality
debug_handler.load_existing_interactions()
```

Stage-based LLM output capture showcases the primary debugging workflow for capturing and organizing LLM interactions by pipeline stage. This pattern enables deterministic debugging with predictable file locations and easy manual inspection capabilities.

```python
# Capture stage-specific LLM output with predictable filename generation
stage = "stage_1_file_analysis"
file_path = Path("src/components/button.py")
prompt = "Analyze this file for architectural patterns..."
response = "## File Analysis\n\nThis component implements..."

debug_handler.capture_stage_llm_output(
    stage=stage,
    prompt=prompt,
    response=response,
    file_path=file_path
)
```

Replay functionality demonstrates the deterministic debugging workflow for reusing previously captured LLM outputs without redundant API calls. This pattern enables efficient debugging iterations and consistent testing scenarios.

```python
# Check for existing debug response before making LLM call
replay_response = debug_handler.get_stage_replay_response(
    stage="stage_1_file_analysis",
    file_path=Path("src/components/button.py")
)

if replay_response:
    # Use saved response for deterministic debugging
    analysis_result = replay_response
else:
    # Make live LLM call and capture result
    analysis_result = await llm_client.analyze(prompt)
    debug_handler.capture_stage_llm_output(stage, prompt, analysis_result, file_path)
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/defaults.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

This file provides centralized default configurations for the Jesse Framework MCP's Knowledge Bases Hierarchical Indexing System, specifically supporting three handler types: `project-base`, `git-clones`, and `pdf-knowledge`. The module enables automatic JSON configuration file generation when configurations don't exist, featuring the `get_default_config()`, `get_supported_handler_types()`, and `validate_handler_type()` functions. Key semantic entities include `PROJECT_BASE_DEFAULT_CONFIG`, `GIT_CLONES_DEFAULT_CONFIG`, `PDF_KNOWLEDGE_DEFAULT_CONFIG`, `BASE_EXCLUDED_EXTENSIONS`, `BASE_EXCLUDED_DIRECTORIES`, `Claude4SonnetModel`, and the `DEFAULT_CONFIGS` registry. The configuration templates provide production-ready defaults with hierarchical exclusion systems, LLM integration via `Claude4SonnetModel.CLAUDE_4_SONNET`, and comprehensive parameter coverage including `file_processing`, `content_filtering`, `llm_config`, `change_detection`, `error_handling`, `output_config`, and `debug_config` sections.

##### Main Components

The file contains three primary configuration dictionaries (`PROJECT_BASE_DEFAULT_CONFIG`, `GIT_CLONES_DEFAULT_CONFIG`, `PDF_KNOWLEDGE_DEFAULT_CONFIG`), base exclusion sets (`BASE_EXCLUDED_EXTENSIONS`, `BASE_EXCLUDED_DIRECTORIES`), project-specific exclusions (`PROJECT_BASE_EXCLUSIONS`), a configuration registry (`DEFAULT_CONFIGS`), and three utility functions (`get_default_config()`, `get_supported_handler_types()`, `validate_handler_type()`). Each configuration template includes seven hierarchical sections covering handler identification, file processing limits, content filtering rules, LLM configuration, change detection settings, error handling parameters, output configuration, and debug options.

###### Architecture & Design

The architecture implements a template-based configuration system with handler-specific optimization and hierarchical exclusion inheritance. The design separates base exclusions (applied universally) from handler-specific exclusions (applied selectively), enabling configuration reuse while maintaining handler specialization. The configuration registry pattern provides centralized access to all handler types through the `DEFAULT_CONFIGS` dictionary, while deep copying ensures template immutability. The hierarchical structure groups related configuration parameters into logical sections (`file_processing`, `content_filtering`, etc.) for improved maintainability and discoverability.

####### Implementation Approach

The implementation uses dictionary-based configuration templates with type annotations and deep copying for safe template access. Each handler configuration optimizes parameters for specific use cases: `project-base` uses larger batch sizes (7 files) and higher file size limits (2MB), `git-clones` uses conservative settings with smaller batches (5 files) and 1MB limits, while `pdf-knowledge` supports larger documents (10MB) with smaller batches (3 files). The exclusion system combines base exclusions with handler-specific additions, and the `copy.deepcopy()` approach prevents accidental template modification during configuration generation.

######## External Dependencies & Integration Points

**→ Inbound:** [dependencies this file requires]
- `jesse_framework_mcp.llm.strands_agent_driver.models:Claude4SonnetModel` - LLM model configuration for all handler types
- `typing` (external library) - type annotations for configuration templates and function signatures
- `copy` (external library) - deep copying functionality for template immutability

**← Outbound:** [systems that consume this file's configurations]
- `knowledge_bases/indexing/` - indexing handlers consume these default configurations
- `*.json` - generated configuration files based on these templates
- Configuration validation systems that use `validate_handler_type()` and `get_supported_handler_types()`

**⚡ System role and ecosystem integration:**
- **System Role**: Central configuration provider for the Jesse Framework MCP's indexing system, serving as the authoritative source for handler default configurations
- **Ecosystem Position**: Core infrastructure component that enables the indexing system's configuration management and auto-generation capabilities
- **Integration Pattern**: Used by indexing handlers during initialization to generate missing JSON configurations, and by validation systems to ensure handler type compliance

######### Edge Cases & Error Handling

The `get_default_config()` function raises `ValueError` for unsupported handler types with clear error messaging including the list of supported types. The configuration templates include comprehensive error handling parameters: `max_retries` (2-5 depending on handler), `retry_delay_seconds` (0.5-2.0), and `continue_on_file_errors: True` for resilient processing. Edge cases addressed include filesystem timestamp comparison tolerance (2-10 seconds), concurrent operation limits (2-3), and file size constraints (1-10MB) tailored to each handler's processing characteristics.

########## Internal Implementation Details

The module maintains backward compatibility with existing `IndexingConfig` structure through complete parameter coverage in all templates. The `BASE_EXCLUDED_DIRECTORIES` includes system directories (`.git`, `__pycache__`, `node_modules`) and the `scratchpad` directory added for universal exclusion. Configuration templates use nested dictionary structures with consistent key naming patterns, and the `DEFAULT_CONFIGS` registry enables O(1) lookup for handler type validation. The deep copying mechanism in `get_default_config()` prevents template pollution while maintaining reference efficiency for read-only operations.

########### Code Usage Examples

This example demonstrates retrieving and customizing a default configuration for the project-base handler. The deep copy ensures template safety while allowing configuration customization.

```python
from jesse_framework_mcp.knowledge_bases.indexing.defaults import get_default_config

# Get default configuration for project-base handler
config = get_default_config("project-base")
config["file_processing"]["batch_size"] = 10  # Customize batch size
config["llm_config"]["temperature"] = 0.5     # Adjust LLM temperature
```

This example shows handler type validation and supported types discovery for configuration management workflows.

```python
from jesse_framework_mcp.knowledge_bases.indexing.defaults import validate_handler_type, get_supported_handler_types

# Validate handler type before processing
if validate_handler_type("git-clones"):
    config = get_default_config("git-clones")

# Get all supported handler types for UI generation
supported_handlers = get_supported_handler_types()  # Returns ['git-clones', 'pdf-knowledge', 'project-base']
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/execution_engine.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

The `ExecutionEngine` class serves as the core execution orchestrator for the Jesse Framework's Plan-then-Execute architecture within the Knowledge Bases Hierarchical Indexing System. This component transforms `ExecutionPlan` objects containing atomic tasks into concrete file system operations, LLM-powered analysis, and knowledge base generation. The engine provides concurrent task execution with dependency resolution, comprehensive progress reporting, and robust error handling for reliable knowledge base construction. Key semantic entities include `ExecutionEngine`, `AtomicTask`, `TaskType`, `ExecutionResults`, `KnowledgeBuilder`, `FileAnalysisCache`, `DirectoryContext`, `FileContext`, `ProcessingStatus`, `IndexingConfig`, and `asyncio` semaphore-based concurrency control. The implementation leverages task dispatch patterns with specialized handlers for nine distinct task types including `ANALYZE_FILE_LLM`, `CREATE_DIRECTORY_KB`, and orphaned file cleanup operations.

##### Main Components

The execution engine contains several primary components: the `ExecutionEngine` class as the main orchestrator, task execution handlers dictionary (`_task_handlers`) mapping `TaskType` enum values to specialized async methods, concurrency control through `_execution_semaphore` limiting parallel operations, execution state tracking sets (`_completed_tasks`, `_failed_tasks`, `_running_tasks`), integrated `KnowledgeBuilder` for LLM-powered content analysis, and `FileAnalysisCache` for performance optimization. The engine includes nine specialized task handlers: `_execute_analyze_file_llm`, `_execute_skip_file_cached`, `_execute_create_directory_kb`, `_execute_skip_directory_fresh`, `_execute_delete_orphaned_file`, `_execute_delete_orphaned_directory`, `_execute_create_cache_structure`, `_execute_verify_cache_freshness`, and `_execute_verify_kb_freshness`. Additional utility components include `_strip_cache_metadata` for content cleaning, `_load_cached_analysis_content` for cache integration, and `_cleanup_execution_resources` for resource management.

###### Architecture & Design

The architecture implements a Plan-then-Execute pattern with atomic task execution ensuring complete task isolation and independent processing. The design uses dependency-aware execution respecting task prerequisites through `_are_dependencies_satisfied` validation, concurrent execution optimization via `asyncio.Semaphore` for resource utilization, and comprehensive progress reporting providing real-time execution status. The system employs a task dispatch pattern routing different `TaskType` values to specialized handlers, maintains execution state through set-based tracking of task completion status, and integrates existing components (`KnowledgeBuilder`, `FileAnalysisCache`) for seamless operation. Error handling enables graceful degradation with configurable `continue_on_file_errors` behavior, while resource management prevents memory leaks through proper cleanup procedures. The architecture supports both preview mode for plan analysis and full execution mode with performance metrics collection.

####### Implementation Approach

The implementation uses `asyncio` for concurrent task execution with semaphore-based concurrency control limiting parallel operations to `config.max_concurrent_operations`. Task execution follows dependency resolution through `plan.get_execution_order()` providing topologically sorted task sequences, with each task validated via `_are_dependencies_satisfied` before execution. The engine employs task dispatch using a handler dictionary mapping `TaskType` enum values to specialized async methods, maintaining execution state through three sets tracking completed, failed, and running tasks. Content processing involves reconstructing `FileContext` and `DirectoryContext` objects from task metadata, loading cached analysis content via `_load_cached_analysis_content`, and stripping metadata contamination through `_strip_cache_metadata`. Performance optimization includes bypass cache flags for stale content, parallel execution opportunities through dependency analysis, and comprehensive metrics collection in `ExecutionResults` objects. Error handling implements configurable failure behavior with detailed logging and graceful task failure recovery.

######## External Dependencies & Integration Points

**→ Inbound:**
- `..models.IndexingConfig` - Configuration parameters for execution behavior and resource limits
- `..models.DirectoryContext` - Directory processing context with file relationships and metadata
- `..models.FileContext` - File processing context with analysis results and status tracking
- `..models.ProcessingStatus` - Enumeration for tracking processing state transitions
- `..models.execution_plan.ExecutionPlan` - Plan objects containing atomic tasks and dependencies
- `..models.execution_plan.AtomicTask` - Individual task definitions with metadata and requirements
- `..models.execution_plan.TaskType` - Task type enumeration for handler dispatch
- `..models.execution_plan.ExecutionResults` - Results collection with performance metrics
- `.knowledge_builder.KnowledgeBuilder` - LLM-powered content analysis and KB generation
- `.file_analysis_cache.FileAnalysisCache` - Caching system for performance optimization
- `fastmcp.Context` - FastMCP context for progress reporting and logging
- `asyncio` (external library) - Async programming patterns and concurrency control
- `pathlib.Path` (external library) - Cross-platform path operations and file metadata
- `logging` (external library) - Structured logging for execution analysis

**← Outbound:**
- `jesse_framework_mcp/knowledge_bases/indexing/hierarchical_indexer.py:HierarchicalIndexer` - Main indexer consuming execution results
- `generated/knowledge_base_files/*.md` - Generated knowledge base files from directory processing
- `generated/cache_files/*.analysis.md` - Cached analysis files from LLM processing
- `logging_system` - Structured execution logs for debugging and monitoring

**⚡ System role and ecosystem integration:**
- **System Role**: Core execution engine transforming declarative execution plans into concrete file system operations and knowledge base generation within the Jesse Framework's hierarchical indexing workflow
- **Ecosystem Position**: Central component bridging high-level planning (`ExecutionPlan`) with low-level operations (`KnowledgeBuilder`, `FileAnalysisCache`) in the Plan-then-Execute architecture
- **Integration Pattern**: Used by `HierarchicalIndexer` for executing complex indexing workflows, consuming plans from planning components, and coordinating with FastMCP for progress reporting to human operators

######### Edge Cases & Error Handling

The engine handles multiple error scenarios including dependency validation failures where tasks cannot execute due to unsatisfied prerequisites, individual task execution failures with configurable `continue_on_file_errors` behavior allowing workflow continuation, and resource exhaustion through semaphore-based concurrency limiting. Cache-related edge cases include missing cached analysis content handled by `_load_cached_analysis_content` with fallback to placeholder text, metadata contamination prevented by `_strip_cache_metadata` stripping XML tags, and stale cache detection triggering LLM re-analysis. File system edge cases cover orphaned file deletion with safety checks via `is_safe_to_delete` metadata, empty directory removal using `rmdir()` for non-empty directory protection, and missing file handling during verification tasks. The system provides comprehensive error reporting through `ExecutionResults.failed_tasks` with detailed error messages, graceful degradation allowing partial execution completion, and proper resource cleanup via `_cleanup_execution_resources` preventing memory leaks. Debugging support includes detailed logging at multiple levels, execution state tracking through running/completed/failed task sets, and preview mode for plan analysis without side effects.

########## Internal Implementation Details

The execution engine maintains internal state through three sets: `_completed_tasks`, `_failed_tasks`, and `_running_tasks` for tracking task lifecycle progression. Concurrency control uses `asyncio.Semaphore(config.max_concurrent_operations)` with async context manager pattern in `_execute_single_task` for resource management. Task metadata reconstruction involves complex object rebuilding from serialized task data, particularly for `DirectoryContext` objects requiring `knowledge_file_path` assignment and `FileContext` objects needing cached content loading. The `_strip_cache_metadata` method uses regex pattern `r'\s*'` with `DOTALL` flag for multiline metadata block removal. Cache integration through `_load_cached_analysis_content` handles file system errors gracefully, returning `None` for missing content to enable fallback behavior. Performance metrics collection in `ExecutionResults` tracks `llm_calls_made`, `files_processed`, `directories_processed`, and `files_deleted` with execution duration calculation. Resource cleanup involves `knowledge_builder.cleanup()` for connection management and execution state clearing for memory management. The task handler dispatch system uses dictionary lookup with runtime error handling for missing handlers, ensuring robust task execution even with configuration errors.

########### Code Usage Examples

**Basic execution engine initialization and plan execution:**
```python
# Initialize execution engine with configuration
config = IndexingConfig(max_concurrent_operations=4, continue_on_file_errors=True)
engine = ExecutionEngine(config)

# Execute a complete plan with progress reporting
async def execute_indexing_plan(plan: ExecutionPlan, ctx: Context):
    results = await engine.execute_plan(plan, ctx)
    print(f"Completed: {len(results.completed_tasks)}, Failed: {len(results.failed_tasks)}")
    print(f"LLM calls: {results.llm_calls_made}, Duration: {results.total_duration:.1f}s")
    return results
```

**Plan preview for debugging and analysis:**
```python
# Preview execution plan without executing tasks
async def preview_execution_strategy(plan: ExecutionPlan, ctx: Context):
    await engine.preview_plan(plan, ctx)
    # Shows task dependencies, execution order, and resource requirements
```

**Custom task execution with error handling:**
```python
# Execute individual tasks with dependency validation
async def execute_with_validation(task: AtomicTask, ctx: Context):
    if engine._are_dependencies_satisfied(task):
        await engine._execute_single_task(task, ctx)
        print(f"Task {task.task_id} completed successfully")
    else:
        print(f"Dependencies not satisfied for {task.task_id}")
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/file_analysis_cache.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

The `FileAnalysisCache` class provides high-performance caching capabilities for individual LLM file analysis outputs within the Jesse Framework MCP knowledge building system. This cache manager eliminates redundant LLM API calls by storing and retrieving analysis results for unchanged source files, implementing timestamp-based staleness detection with `is_cache_fresh()` method returning detailed reasoning tuples. Key semantic entities include `FileAnalysisCache` class, `IndexingConfig` configuration model, `DirectoryContext` and `FileContext` data structures, `get_portable_path()` utility function, HTML comment metadata delimiters (`METADATA_START`, `METADATA_END`), project-base directory mirroring strategy, `.analysis.md` cache file suffix, `prepare_cache_structure()` method for concurrent operation safety, and `RebuildDecisionEngine` integration for staleness determination. The system provides clean metadata separation ensuring no cache artifacts contaminate final knowledge files through `_extract_analysis_content()` method, comprehensive error handling with graceful degradation on cache failures, and rich debugging capabilities with detailed timestamp comparisons and constituent staleness analysis.

##### Main Components

The module contains the primary `FileAnalysisCache` class with core caching operations including `get_cached_analysis()` for retrieval, `cache_analysis()` for storage, and `is_cache_fresh()` for staleness checking. Cache path management is handled by `get_cache_path()` following project-base directory structure mirroring. Knowledge file operations include `get_knowledge_file_path()` and `is_knowledge_file_stale()` for directory-level rebuild decisions. Metadata management components include `_create_metadata_header()` for rich cache tracking and `_extract_analysis_content()` for clean content extraction. Utility methods provide `get_constituent_staleness_info()` for detailed debugging analysis, `prepare_cache_structure()` for concurrent operation safety, `clear_cache()` for cache invalidation, and `get_cache_stats()` for monitoring. Helper methods include `_collect_all_files_recursive()` for directory traversal and `_is_handler_root_directory()` for consistent path generation across knowledge base handler types.

###### Architecture & Design

The architecture implements a cache-first processing strategy with clean metadata separation using HTML comment delimiters to prevent cache artifacts from contaminating final knowledge files. The design follows project-base directory structure mirroring as mandated by indexing business rules, creating `.analysis.md` cache files that mirror source file organization. Timestamp-based freshness checking uses direct filesystem timestamp comparison without tolerance for reliability and simplicity. The system employs upfront cache structure preparation through `prepare_cache_structure()` to eliminate race conditions during concurrent operations. Error handling follows graceful degradation principles where cache failures never break core knowledge building processes. The metadata system uses structured HTML comments with `CACHE_VERSION` for future compatibility and includes portable path references using `get_portable_path()` for cross-environment compatibility.

####### Implementation Approach

The implementation uses direct timestamp comparison between source files and cache files without tolerance, where cache is fresh if `cache_mtime >= source_mtime`. Cache files combine metadata headers with analysis content using clear HTML comment delimiters for reliable extraction. The system implements two-layer processing flow: source files trigger individual file reprocessing when changed, while knowledge files rebuild only when cached analyses or subdirectory knowledge files are newer. Directory structure preparation uses recursive `DirectoryContext` traversal to identify all files requiring cache directories, then creates unique cache directory sets atomically. Path calculation follows project-base indexing rules with relative path preservation and standardized `.analysis.md` suffix application. The staleness checking system specifically excludes cached analyses from directory rebuild decisions to prevent infinite rebuild loops, focusing only on source files and subdirectory knowledge files for directory-level staleness determination.

######## External Dependencies & Integration Points

**→ Inbound:**
- `..models.IndexingConfig` - Configuration model providing cache paths and timestamp tolerance settings
- `..models.DirectoryContext` - Hierarchical directory structure representation for cache preparation
- `..models.FileContext` - Individual file metadata with timestamps for staleness checking
- `...helpers.path_utils:get_portable_path` - Portable path generation using JESSE path variables
- `fastmcp.Context` - MCP framework context for progress reporting and user communication
- `pathlib.Path` (standard library) - Cross-platform path operations and file metadata access
- `datetime.datetime` (standard library) - Timestamp comparison and cache freshness determination
- `logging` (standard library) - Structured logging for cache operations and debugging

**← Outbound:**
- `knowledge_building/rebuild_decision_engine.py` - Consumes staleness checking methods for rebuild decisions
- `knowledge_building/knowledge_builder.py` - Uses cache retrieval and storage during file processing
- `indexing/hierarchical_indexer.py` - Integrates cache structure preparation in processing workflow
- `{PROJECT_ROOT}/jesse-framework-mcp/knowledge-base/project-base/` - Generated cache files with metadata headers

**⚡ System role and ecosystem integration:**
- **System Role**: Core performance optimization component in the Jesse Framework MCP knowledge building pipeline, serving as the primary cache layer between source file analysis and knowledge file generation
- **Ecosystem Position**: Central infrastructure component that significantly impacts system performance by reducing LLM API costs and processing time through intelligent caching
- **Integration Pattern**: Used by knowledge builders during file processing workflows, integrated with rebuild decision engines for staleness determination, and consumed by hierarchical indexers for structure preparation during batch operations

######### Edge Cases & Error Handling

The system handles missing cache files by returning `None` from `get_cached_analysis()` to trigger fresh LLM analysis. Filesystem access errors during timestamp comparison result in conservative staleness assumptions, treating files as requiring fresh analysis. Cache write failures are logged but never propagate to break core knowledge building processes. Path calculation errors fall back to flat structure in project-base directory to ensure cache operations continue. Metadata extraction failures return original content for backward compatibility with cache files lacking metadata delimiters. Concurrent access scenarios are handled through upfront directory structure preparation, with fallback to on-demand directory creation if preparation fails. The system treats cache freshness check failures conservatively by assuming staleness to trigger rebuilds rather than risk serving stale content. Directory structure preparation failures are logged but don't prevent processing, as individual cache operations fall back to on-demand directory creation.

########## Internal Implementation Details

Cache files use HTML comment metadata blocks with `CACHE_VERSION = "1.0"` for future compatibility and migration support. The metadata header includes portable source file paths using `get_portable_path()`, cache timestamp, source modification time, and cache version. Content extraction uses string operations to locate `METADATA_START` and `METADATA_END` delimiters, removing everything up to and including the end delimiter. Timestamp tolerance is calculated from `IndexingConfig.timestamp_tolerance_seconds` but currently unused in direct comparison logic. The `_is_handler_root_directory()` method implements identical logic to `KnowledgeBuilder` for consistent path generation across project-base, git-clones, and PDF-knowledge handlers. Cache statistics collection traverses the cache directory using `rglob("*.analysis.md")` pattern matching. The constituent staleness analysis builds comprehensive dictionaries with ISO timestamp formatting and detailed reasoning for debugging purposes. Directory structure preparation uses set operations to eliminate duplicate directory creation and batch processing for atomic structure creation.

########### Code Usage Examples

**Basic cache retrieval and storage pattern:**
```python
# Initialize cache with configuration
cache = FileAnalysisCache(config)

# Check for cached analysis
cached_content = await cache.get_cached_analysis(file_path, source_root)
if cached_content:
    # Use cached analysis directly
    analysis_result = cached_content
else:
    # Perform fresh LLM analysis
    analysis_result = await perform_llm_analysis(file_path)
    # Cache the result for future use
    await cache.cache_analysis(file_path, analysis_result, source_root)
```

**Cache structure preparation for concurrent operations:**
```python
# Prepare entire cache structure upfront to eliminate race conditions
await cache.prepare_cache_structure(root_context, source_root, ctx)

# Now safe to perform concurrent caching operations
tasks = [cache.cache_analysis(file_path, analysis, source_root) 
         for file_path, analysis in file_analyses]
await asyncio.gather(*tasks)
```

**Knowledge file staleness checking:**
```python
# Check if directory knowledge file needs rebuilding
is_stale, reason = cache.is_knowledge_file_stale(
    directory_path, source_root, file_contexts, subdirectory_paths
)
if is_stale:
    logger.info(f"Rebuilding knowledge file: {reason}")
    # Trigger knowledge file rebuild
```

**Detailed staleness analysis for debugging:**
```python
# Get comprehensive staleness information
staleness_info = await cache.get_constituent_staleness_info(
    directory_path, source_root, file_contexts, subdirectory_paths, ctx
)
# Access detailed timestamp comparisons and reasoning
for file_info in staleness_info['source_files']:
    print(f"File: {file_info['name']}, Modified: {file_info['mtime']}")
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/hierarchical_indexer.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

This module implements the core orchestrator for hierarchical knowledge base indexing using a Plan-then-Execute architecture that separates decision-making from execution for perfect debuggability. The functional intent centers on coordinating leaf-first processing strategies to build hierarchical knowledge files throughout directory structures using bottom-up assembly approaches. Key semantic entities include `HierarchicalIndexer` class implementing the main orchestration logic, `RebuildDecisionEngine` for centralized decision-making, `PlanGenerator` for converting decisions into atomic tasks, `ExecutionEngine` for dependency-aware task execution, and integration with `FastMCP` `Context` for real-time progress reporting. The module provides comprehensive change detection through `DirectoryContext` and `FileContext` models, supports concurrent processing with configurable limits, and implements defensive error handling enabling graceful degradation and partial processing recovery for large-scale knowledge base maintenance operations.

##### Main Components

The module contains the `HierarchicalIndexer` class as the primary orchestrator with methods for complete indexing workflow coordination, `index_hierarchy()` method implementing the five-phase Plan-then-Execute architecture, `_discover_directory_structure()` and `_build_directory_context()` methods for recursive directory structure discovery, `_detect_changes()` and `_apply_comprehensive_change_detection()` methods for change detection using `RebuildDecisionEngine` integration, `_generate_execution_plan()`, `_preview_execution_plan()`, and `_execute_plan_with_progress()` methods for atomic task planning and execution, and utility methods including `_get_all_directories()` for hierarchy traversal and `_create_final_status()` for result mapping to `IndexingStatus` format.

###### Architecture & Design

The architecture implements Plan-then-Execute pattern separating decision-making from execution through distinct phases: Discovery builds complete `DirectoryContext` hierarchy, Decision Analysis generates comprehensive `DecisionReport` with change detection, Plan Generation converts decisions into atomic `ExecutionPlan` with dependencies, Plan Preview provides detailed execution analysis for debuggability, and Atomic Execution performs dependency-aware task execution. Design principles emphasize leaf-first hierarchical processing ensuring child completion before parent processing, bottom-up assembly aggregating child summaries into parent knowledge files, async-first architecture supporting concurrent operations with `FastMCP` `Context` integration, modular component delegation to specialized handlers, and defensive programming with comprehensive error handling and recovery mechanisms.

####### Implementation Approach

The implementation utilizes recursive directory discovery building complete hierarchy context through `_build_directory_context()` with configuration filtering via `should_process_file()` and `should_process_directory()` methods. Change detection employs `RebuildDecisionEngine.should_rebuild_directory()` for centralized decision-making with comprehensive constituent dependency checking. The Plan-then-Execute workflow delegates to `PlanGenerator.create_execution_plan()` for atomic task generation, `ExecutionEngine.preview_plan()` for detailed execution analysis, and `ExecutionEngine.execute_plan()` for dependency-aware task execution. Processing coordination maintains `IndexingStatus` with real-time progress updates, `ProcessingStats` for performance metrics, and error handling enabling graceful degradation through configurable `continue_on_file_errors` behavior.

######## External Dependencies & Integration Points

**→ Inbound:**
- `..models.IndexingConfig` - Configuration and filtering logic for processing rules and limits
- `..models.DirectoryContext` - Directory structure representation with processing status tracking
- `..models.FileContext` - File metadata and processing state management
- `..models.ProcessingStatus` - Status enumeration for processing state tracking
- `..models.IndexingStatus` - Comprehensive indexing operation status reporting
- `.rebuild_decision_engine.RebuildDecisionEngine` - Centralized decision-making for change detection
- `.knowledge_builder.KnowledgeBuilder` - LLM-powered content summarization and knowledge file generation
- `.special_handlers.ProjectBaseHandler` - Project-base specialized processing for whole codebase indexing
- `.special_handlers.GitCloneHandler` - Git-clone specialized processing for read-only repository handling
- `.plan_generator.PlanGenerator` - Decision-to-task conversion for atomic execution planning
- `.execution_engine.ExecutionEngine` - Dependency-aware atomic task execution with progress reporting
- `fastmcp.Context` (external library) - Real-time progress reporting and user interaction
- `asyncio` (standard library) - Async programming patterns and concurrency control
- `pathlib.Path` (standard library) - Cross-platform path operations and filesystem interaction

**← Outbound:**
- Knowledge base indexing tools consuming `HierarchicalIndexer.index_hierarchy()` method
- MCP server implementations requiring hierarchical processing coordination
- JESSE Framework components needing structured knowledge base maintenance
- Monitoring systems accessing `current_status` property for real-time progress tracking

**⚡ System role and ecosystem integration:**
- **System Role**: Serves as the central orchestrator for the Knowledge Bases Hierarchical Indexing System, coordinating all phases of directory discovery, change detection, plan generation, and atomic task execution
- **Ecosystem Position**: Core component bridging high-level indexing requests with specialized processing engines, decision systems, and execution frameworks
- **Integration Pattern**: Consumed by MCP tools requiring hierarchical knowledge base maintenance, integrating with specialized handlers for git-clones and project-base scenarios, and coordinating with LLM-powered content generation through structured Plan-then-Execute workflows

######### Edge Cases & Error Handling

Error handling implements comprehensive exception catching with detailed logging through `logger.error()` calls including stack traces and graceful degradation options. Edge cases include filesystem access failures handled through `OSError` and `PermissionError` catching with continued processing, change detection failures triggering conservative fallback marking directories for processing, execution failures managed through configurable `continue_on_file_errors` behavior, and resource cleanup errors handled gracefully in `cleanup()` method. The system provides defensive programming patterns including validation of root path existence and directory status, comprehensive error statistics tracking through `ProcessingStats.add_error()`, and status determination based on execution success rates with configurable thresholds for partial success scenarios.

########## Internal Implementation Details

Internal mechanisms utilize `datetime.now()` for processing timing and performance metrics, recursive directory traversal through `_get_all_directories()` for comprehensive hierarchy analysis, and status mapping between `ExecutionResults` and `IndexingStatus` formats for API compatibility. The implementation maintains processing coordination through `_current_status` updates with real-time operation tracking, component initialization with dependency injection for `RebuildDecisionEngine`, `PlanGenerator`, and `ExecutionEngine`, and resource management through `cleanup()` method delegating to `ExecutionEngine._cleanup_execution_resources()`. Processing statistics include accurate file and directory counts, error tracking with detailed messages, and performance metrics including LLM call counts and execution duration measurements.

########### Code Usage Examples

**Basic hierarchical indexing initialization and execution:** This example demonstrates how to initialize the indexer with configuration and execute hierarchical processing with progress reporting capabilities.

```python
from pathlib import Path
from fastmcp import Context
from jesse_framework_mcp.knowledge_bases.models import IndexingConfig
from jesse_framework_mcp.knowledge_bases.indexing.hierarchical_indexer import HierarchicalIndexer

# Initialize indexer with configuration
config = IndexingConfig()
indexer = HierarchicalIndexer(config)

# Execute hierarchical indexing with progress reporting
async def run_indexing():
    ctx = Context()
    root_path = Path(".knowledge/")
    status = await indexer.index_hierarchy(root_path, ctx)
    return status
```

**Real-time status monitoring during indexing operations:** This pattern enables monitoring of indexing progress with real-time status updates and progress percentage tracking.

```python
# Monitor indexing progress in real-time
async def monitor_indexing_progress(indexer):
    while indexer.current_status.overall_status == ProcessingStatus.PROCESSING:
        status = indexer.current_status
        print(f"Operation: {status.current_operation}")
        print(f"Progress: {status.processing_stats.progress_percentage:.1f}%")
        await asyncio.sleep(1)
```

**Plan-then-Execute architecture component access:** This example shows how to access the internal components of the Plan-then-Execute architecture and perform proper resource cleanup.

```python
# Access Plan-then-Execute architecture components
indexer = HierarchicalIndexer(config)
decision_engine = indexer.rebuild_decision_engine
plan_generator = indexer.plan_generator
execution_engine = indexer.execution_engine

# Cleanup resources after processing
await indexer.cleanup()
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/knowledge_builder.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

This file implements a comprehensive LLM-powered knowledge file generation system using `Claude4SonnetConfig` for hierarchical semantic analysis and structured knowledge base creation. The `KnowledgeBuilder` class provides cache-first file analysis through `FileAnalysisCache`, continuation-based retry mechanisms with `StrandsClaude4Driver`, and template-driven knowledge file assembly via `KnowledgeFileGenerator`. Key semantic entities include `TruncationDetectedError` for artifact prevention, `EnhancedPrompts` for specialized analysis workflows, `DebugHandler` for replay functionality, and `ProcessingStatus` enumeration for workflow state management. The system implements a 3-phase generation workflow: individual file analysis with factual LLM processing, programmatic content insertion and subdirectory assembly, and global summary generation using assembled content for comprehensive synthesis.

##### Main Components

The file contains the `KnowledgeBuilder` class as the primary orchestrator, `TruncationDetectedError` custom exception for truncation handling, and multiple private methods for specialized processing workflows. Core processing methods include `build_file_knowledge()` for individual file analysis, `build_directory_summary()` for hierarchical directory processing, and `_process_single_file()` for cache-first LLM analysis. Content generation methods encompass `_generate_global_summary()` for directory-level synthesis, `_extract_subdirectory_content()` for filtered content extraction, and `_generate_global_summary_from_contexts()` for comprehensive context assembly. Utility methods provide `_retry_llm_call_with_truncation_check()` for robust LLM communication, `_review_content_until_compliant()` for quality assurance, and `_merge_responses()` for intelligent response combination.

###### Architecture & Design

The system follows a hierarchical bottom-up assembly pattern where individual files are analyzed first, then aggregated into directory summaries, and finally synthesized into global knowledge files. The architecture implements a dual content strategy using full subdirectory content for LLM context and filtered content for final knowledge base files to prevent truncation issues. Cache-first processing strategy maximizes performance through `FileAnalysisCache` integration with selective bypass mechanisms for rebuild scenarios. The design incorporates a bounded loop reviewer workflow with dual truncation detection combining programmatic marker checks and LLM reviewer validation. Error handling follows a fail-fast approach with `TruncationDetectedError` preventing any artifact creation when truncation is detected, ensuring knowledge base integrity.

####### Implementation Approach

The implementation uses continuation-based retry mechanisms that maintain conversation context across truncation recovery attempts, providing 90%+ token savings compared to fresh conversation approaches. Content processing employs intelligent response merging with overlap detection and duplicate sentence removal for seamless content combination. The system implements a triple detection strategy for truncation: programmatic marker detection, LLM reviewer validation, and reviewer correction truncation checks. File analysis utilizes content chunking for large files exceeding LLM context window constraints, with specialized prompts based on content-type detection. Directory processing follows a centralized decision engine pattern where `RebuildDecisionEngine` determines processing requirements, and `KnowledgeBuilder` executes template generation with alphabetical sorting and full rebuild approaches.

######## External Dependencies & Integration Points

**→ Inbound:**
- `..models.IndexingConfig` - configuration parameters for LLM model settings and processing options
- `..models.DirectoryContext` - directory processing state and file context aggregation
- `..models.FileContext` - individual file processing metadata and knowledge content storage
- `..models.ProcessingStatus` - enumeration for workflow state management and error tracking
- `...llm.strands_agent_driver.StrandsClaude4Driver` - Claude 4 Sonnet LLM integration for content analysis
- `...llm.strands_agent_driver.Claude4SonnetConfig` - LLM configuration optimization for analysis tasks
- `.knowledge_file_generator.KnowledgeFileGenerator` - template engine for structured knowledge file creation
- `.knowledge_prompts.EnhancedPrompts` - specialized prompt generation for different analysis contexts
- `.debug_handler.DebugHandler` - debug capture and replay functionality for development workflows
- `.file_analysis_cache.FileAnalysisCache` - performance optimization through timestamp-based cache management

**← Outbound:**
- `hierarchical_indexer.py:HierarchicalIndexer` - primary consumer for directory knowledge building workflows
- `knowledge_base_handlers/` - handler implementations consuming knowledge building services for different content types
- `debug_output/` - debug interaction capture files for replay and analysis workflows
- `cache/` - cached analysis files for performance optimization and staleness detection

**⚡ System role and ecosystem integration:**
- **System Role**: Core knowledge generation engine within the Jesse Framework MCP server, responsible for transforming raw file content into structured hierarchical knowledge bases through LLM analysis
- **Ecosystem Position**: Central component bridging file system analysis with LLM processing capabilities, serving as the primary content transformation layer
- **Integration Pattern**: Used by `HierarchicalIndexer` for bottom-up knowledge assembly, consumed by knowledge base handlers for different content types, and integrated with caching and debug systems for performance and development workflows

######### Edge Cases & Error Handling

The system implements comprehensive truncation detection through `TruncationDetectedError` which completely prevents artifact creation when LLM responses are incomplete, ensuring knowledge base integrity. Empty file handling gracefully skips unreadable or zero-length files with appropriate status tracking through `ProcessingStatus.SKIPPED`. Binary file detection prevents processing of unsuitable content through encoding error handling and null byte detection. Technical LLM errors are distinguished from content errors, with technical failures triggering file skipping while content issues allow caching of best-attempt results. The bounded loop reviewer prevents infinite loops through maximum iteration limits while preserving LLM work through forced caching when max iterations are reached. Cache staleness verification includes algorithm bug detection that raises runtime errors when freshness expectations are violated after rebuild operations.

########## Internal Implementation Details

The cache-first processing strategy checks `FileAnalysisCache` before LLM calls unless bypass conditions are met through rebuild decisions or full indexing mode. Conversation ID generation uses normalized path patterns with UUID suffixes for unique LLM conversation contexts. Content extraction employs header filtering to remove LLM conversational artifacts while preserving legitimate section structure through pattern recognition. The dual content strategy loads full KB content for LLM global summary context while extracting filtered fourth-level header content for final KB integration. Response merging implements sentence-level overlap detection with boundary analysis and duplicate removal for seamless content combination. Debug capture follows structured stage naming with iteration tracking for comprehensive replay functionality. Knowledge file path determination implements handler root detection for proper `root_kb.md` vs `{directory_name}_kb.md` naming conventions.

########### Code Usage Examples

**Basic knowledge builder initialization and file processing:**
```python
# Initialize knowledge builder with configuration
config = IndexingConfig(llm_model="claude-3-5-sonnet-20241022", debug_mode=True)
builder = KnowledgeBuilder(config)
await builder.initialize()

# Process individual file with cache-first approach
file_context = FileContext(file_path=Path("src/module.py"), file_size=5000, last_modified=datetime.now())
result = await builder.build_file_knowledge(file_context, ctx, source_root=Path("/project"))
```

**Directory knowledge building with hierarchical assembly:**
```python
# Build directory summary with subdirectory aggregation
directory_context = DirectoryContext(
    directory_path=Path("src/components/"),
    file_contexts=[completed_file_contexts],
    subdirectory_contexts=[completed_subdir_contexts]
)
result = await builder.build_directory_summary(directory_context, ctx, source_root)
```

**Error handling and truncation detection:**
```python
try:
    knowledge_content = await builder.build_file_knowledge(file_context, ctx)
except TruncationDetectedError as e:
    # Truncation detected - no artifacts created, file completely skipped
    logger.error(f"File skipped due to truncation: {e}")
    return None  # Indicates complete file omission from processing
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/knowledge_file_generator.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

This module provides template-based knowledge file generation using a full rebuild approach that replaces complex incremental updates with straightforward string template generation and alphabetical sorting. The functional intent centers on generating complete knowledge base files from scratch on every change, eliminating complex parsing and section replacement logic in favor of predictable, debuggable output. Key semantic entities include `KnowledgeFileGenerator` class implementing the main template generation logic, `generate_complete_knowledge_file()` method for full file creation, `FileContext` and `DirectoryContext` models for content organization, `get_portable_path()` utility for cross-platform path handling, and integration with `CommonMark` specification compliance for maximum markdown compatibility. The module implements alphabetical sorting for consistent file and subdirectory ordering, preserves LLM formatting through direct content insertion without transformation, and provides centralized decision logic relying on `RebuildDecisionEngine` for all rebuild decisions.

##### Main Components

The module contains the `KnowledgeFileGenerator` class as the primary template generator with `generate_complete_knowledge_file()` method for complete file creation from components, `_generate_warning_header()` method for consistent file headers preventing manual editing, `_generate_subdirectory_section()` and `_generate_file_section()` methods for formatted content sections, `_generate_metadata_footer()` method for comprehensive file metadata, `_generate_timestamp()` method for standardized timestamp formatting, and utility methods including `_should_process_file()` for file type filtering and `_get_kb_path()` for knowledge base file path generation following naming conventions.

###### Architecture & Design

The architecture implements full rebuild pattern generating complete knowledge files from templates on every change rather than incremental updates. Design principles emphasize template-based generation using string templates for predictable and debuggable output, alphabetical sorting ensuring consistent file and subdirectory ordering across all knowledge files, content preservation maintaining LLM formatting through direct insertion without transformation, and centralized decision logic trusting decisions already made by `RebuildDecisionEngine`. The structure separates template generation concerns from parsing complexity, uses deterministic output generation for reproducible results, and implements cross-platform compatibility through portable path handling and case-insensitive sorting.

####### Implementation Approach

The implementation utilizes string template assembly building complete knowledge files through concatenated content parts with consistent structure. Content sorting employs alphabetical ordering using `sorted()` with case-insensitive key functions for files and subdirectories before template generation. Template generation creates structured markdown with warning headers, global summaries, subdirectory integration sections, file integration sections, and metadata footers. Path handling uses `get_portable_path()` utility for cross-platform compatibility with proper trailing slash handling for directories. Error handling implements comprehensive exception catching with detailed logging and graceful degradation providing fallback content when individual sections fail.

######## External Dependencies & Integration Points

**→ Inbound:**
- `..models.knowledge_context.FileContext` - File metadata and knowledge content structure for template generation
- `..models.knowledge_context.DirectoryContext` - Directory structure representation with processing context
- `...helpers.path_utils.get_portable_path` - Cross-platform path conversion utility for consistent path formatting
- `pathlib.Path` (standard library) - Cross-platform path operations and metadata handling
- `datetime.datetime` (standard library) - Timestamp generation for content tracking and metadata
- `typing.List` (standard library) - Type hints for template parameters and content structures

**← Outbound:**
- Knowledge base indexing systems consuming `generate_complete_knowledge_file()` method output
- File system operations writing generated markdown content to knowledge base files
- Template validation systems requiring `CommonMark` specification compliance
- Content management systems processing generated knowledge base files

**⚡ System role and ecosystem integration:**
- **System Role**: Serves as the final content generation layer in the Knowledge Bases Hierarchical Indexing System, converting structured context data into formatted markdown knowledge files
- **Ecosystem Position**: Core content generation component bridging structured data models with filesystem output, replacing complex incremental update engines with straightforward template generation
- **Integration Pattern**: Consumed by hierarchical indexers requiring complete knowledge file generation, integrating with portable path utilities for cross-platform compatibility, and producing markdown output consumed by knowledge base management systems

######### Edge Cases & Error Handling

Error handling implements comprehensive exception catching with detailed logging through `logger.error()` calls and graceful degradation providing fallback content when template generation fails. Edge cases include portable path conversion failures handled with fallback to basic path strings and continued processing, file extension detection using case-insensitive comparison for cross-platform compatibility, empty content scenarios providing placeholder text when summaries or analysis content are unavailable, and metadata generation failures providing minimal footer information with error indication. The system provides defensive programming patterns including validation of file processability through extension checking, timestamp generation fallbacks using standard ISO format, and template assembly error recovery maintaining partial content generation when individual sections fail.

########## Internal Implementation Details

Internal mechanisms utilize `datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")` for standardized timestamp generation in ISO 8601 format, alphabetical sorting through `sorted()` with `key=lambda f: f.file_path.name.lower()` for case-insensitive ordering, and string template assembly using list concatenation with `"\n".join(content_parts)` for efficient content building. The implementation maintains processable file extensions set including source code, documentation, and configuration file types, portable path handling with trailing slash detection and platform-specific path separator handling, and content preservation through direct string insertion without parsing or transformation. Template structure includes HTML comment warnings, markdown headers with portable paths, content sections with timestamp metadata, and comprehensive footers with generation statistics and file identification.

########### Code Usage Examples

**Complete knowledge file generation from directory context:** This example demonstrates how to generate a complete knowledge base file from directory components with alphabetical sorting and template formatting.

```python
from pathlib import Path
from jesse_framework_mcp.knowledge_bases.indexing.knowledge_file_generator import KnowledgeFileGenerator
from jesse_framework_mcp.knowledge_bases.models.knowledge_context import FileContext, DirectoryContext

# Initialize generator and create complete knowledge file
generator = KnowledgeFileGenerator()
directory_path = Path("src/")
kb_file_path = Path("src_kb.md")

complete_content = generator.generate_complete_knowledge_file(
    directory_path=directory_path,
    global_summary="Directory contains core application modules",
    file_contexts=[file_context1, file_context2],
    subdirectory_summaries=[(Path("utils/"), "Utility functions summary")],
    kb_file_path=kb_file_path
)
```

**File type filtering for processable content:** This pattern shows how to filter files for knowledge base processing based on extension and characteristics.

```python
# Filter files for knowledge base processing
generator = KnowledgeFileGenerator()
processable_files = []

for file_path in directory_path.iterdir():
    if generator._should_process_file(file_path):
        processable_files.append(file_path)
        
print(f"Found {len(processable_files)} processable files")
```

**Knowledge base file path generation following naming conventions:** This example demonstrates consistent KB file naming and location patterns for directory-adjacent placement.

```python
# Generate knowledge base file paths following conventions
generator = KnowledgeFileGenerator()
source_directory = Path("components/")
kb_path = generator._get_kb_path(source_directory)

print(f"KB file will be created at: {kb_path}")  # components_kb.md
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/knowledge_prompts.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

This file provides specialized LLM prompt templates for generating hierarchical semantic trees within the Jesse Framework MCP's knowledge base indexing system, focusing on architectural analysis and technical implementation details. The file implements comprehensive prompt engineering for different content types with token-efficient structure and programmatic content extraction capabilities. Key semantic entities include `EnhancedPrompts` class for prompt orchestration, `file_analysis_prompt` for individual file analysis, `directory_analysis_prompt` for module architecture analysis, `global_summary_prompt` for system-wide synthesis, reviewer prompts for structural compliance validation, `SEMANTIC_ENTITY_USAGE_SPEC` for technical terminology standardization, `LEVEL_8_FORMATTING_SPEC` for dependency formatting, and `get_portable_path` integration for cross-platform compatibility. The implementation emphasizes architectural focus over generic functionality descriptions, ensuring comprehensive coverage of design patterns, implementation strategies, and technical decision-making insights.

##### Main Components

The file contains the `EnhancedPrompts` class as the primary prompt container with specialized templates for different analysis scenarios. Core prompt templates include `file_analysis_prompt` for comprehensive file analysis with architectural emphasis, `directory_analysis_prompt` for module organization and design relationships, and `global_summary_prompt` for system-wide architectural synthesis. Supporting components include three reviewer prompts (`file_analysis_reviewer_prompt`, `directory_analysis_reviewer_prompt`, `global_summary_reviewer_prompt`) for structural compliance validation, shared specifications (`SEMANTIC_ENTITY_USAGE_SPEC`, `LEVEL_8_FORMATTING_SPEC`) for consistent formatting requirements, and utility methods for prompt generation with portable path support and error handling.

###### Architecture & Design

The architecture implements a template-based prompt generation system with content-type specialization and structured response format requirements. The design separates concerns through specialized prompt templates for different analysis contexts while maintaining consistent hierarchical semantic tree structure across all generated content. The component uses composition patterns with shared specification constants (`SEMANTIC_ENTITY_USAGE_SPEC`, `LEVEL_8_FORMATTING_SPEC`) to ensure DRY principles and consistent formatting requirements. The architecture includes comprehensive reviewer prompt integration for automated structural compliance checking, enabling quality assurance without manual intervention. The design emphasizes architectural analysis focus through specialized prompt engineering that prioritizes design patterns, implementation strategies, and technical depth over generic descriptions.

####### Implementation Approach

The implementation uses template string formatting with placeholder substitution for dynamic prompt generation, incorporating file metadata, content, and portable path conversion for cross-platform compatibility. The approach employs hierarchical semantic tree specification with 8 distinct levels (4-11) using markdown headers, ensuring progressive completeness without information redundancy between levels. Technical strategies include semantic entity usage specification requiring backquote formatting for technical terms, external dependencies formatting with standardized visual symbols (→, ←, ⚡), and comprehensive error handling with logging integration. The system implements reviewer prompt patterns for structural compliance validation, supporting both compliance verification and automatic correction in single operations through binary output formats.

######## External Dependencies & Integration Points

**→ Inbound:**
- `...helpers.path_utils:get_portable_path` - cross-platform path conversion for LLM prompt compatibility
- `logging` (external library) - structured logging for prompt generation operations and debugging
- `typing` (external library) - type hints for prompt template parameters and response structures
- `pathlib.Path` (external library) - modern path handling for file and directory operations

**← Outbound:**
- `jesse_framework_mcp.knowledge_bases.indexing.knowledge_file_generator` - knowledge file generator consuming prompt templates for content generation
- LLM processing systems - structured prompts for hierarchical semantic tree generation
- Knowledge base storage systems - generated analysis content following hierarchical semantic tree structure
- Template engine operations - programmatic content extraction and markdown integration

**⚡ System role and ecosystem integration:**

- **System Role**: This component serves as the prompt engineering foundation for the Jesse Framework MCP knowledge base system, providing specialized LLM templates that generate structured architectural analysis content
- **Ecosystem Position**: Core infrastructure component within the knowledge indexing architecture, essential for generating consistent, high-quality technical documentation through LLM processing
- **Integration Pattern**: Used by knowledge file generators and indexing workflows to create structured prompts for LLM analysis, with reviewer prompts ensuring consistent formatting compliance across all generated knowledge base content

######### Edge Cases & Error Handling

The prompt generation system handles comprehensive error scenarios including portable path conversion failures with fallback to original paths, prompt template formatting errors with detailed exception logging, and reviewer prompt generation failures with runtime error propagation. Error handling implements individual operation isolation through try-catch blocks around each prompt generation method, preventing single failures from affecting overall prompt system functionality. The system manages edge cases like missing file content, invalid directory structures, and malformed assembled content through defensive programming patterns and comprehensive logging. Recovery mechanisms include graceful degradation for path conversion failures, detailed error messages for debugging prompt generation issues, and comprehensive exception handling with proper error propagation for integration debugging.

########## Internal Implementation Details

Internal mechanisms include template string formatting with named placeholders for dynamic content insertion, portable path conversion integration using `get_portable_path()` for cross-platform compatibility, and comprehensive logging integration for debugging and monitoring prompt generation operations. The `EnhancedPrompts` class initializes with all prompt templates as instance attributes, enabling efficient reuse and consistent formatting across multiple prompt generation calls. Prompt templates include extensive specification sections for hierarchical semantic tree structure, semantic entity usage requirements, and external dependencies formatting with standardized visual symbols. The implementation uses f-string formatting for specification injection into prompt templates, ensuring DRY principles and consistent formatting requirements across all generated prompts.

########### Code Usage Examples

Basic prompt generation demonstrates comprehensive file analysis with architectural focus. This pattern shows how to initialize the prompt system and generate structured analysis prompts for LLM processing.

```python
# Initialize enhanced prompts with architectural analysis templates
prompts = EnhancedPrompts()

# Generate file analysis prompt with portable path support
file_prompt = prompts.get_file_analysis_prompt(
    file_path=Path("src/module.py"),
    file_content=file_content,
    file_size=1024
)

# Process with LLM for hierarchical semantic tree generation
analysis_result = await llm_client.generate(file_prompt)
```

Directory analysis prompt generation shows module architecture focus. This approach enables comprehensive analysis of directory structures and component relationships within the codebase.

```python
# Generate directory analysis prompt with child content synthesis
directory_prompt = prompts.get_directory_analysis_prompt(
    directory_path=Path("src/components/"),
    file_count=15,
    subdirectory_count=3,
    child_content_summary=assembled_child_content
)

# Process for comprehensive module architecture analysis
directory_analysis = await llm_client.generate(directory_prompt)
```

Reviewer prompt integration enables automated structural compliance checking. This pattern ensures consistent formatting across all generated knowledge base content through automated validation and correction.

```python
# Generate reviewer prompt for structural compliance validation
reviewer_prompt = prompts.get_file_analysis_reviewer_prompt(generated_output)

# Process for automatic formatting correction
review_result = await llm_client.generate(reviewer_prompt)

# Handle compliance result (either "COMPLIANT" or corrected version)
if review_result == "COMPLIANT":
    final_content = generated_output
else:
    final_content = review_result
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/markdown_parser.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

This file implements AST-based markdown parsing and manipulation using `mistletoe` for reliable document structure understanding and safe content editing without fragile placeholder-based approaches. It provides header-based section identification, content insertion, replacement, and spacing preservation capabilities for existing markdown files. The system enables safe editing of knowledge base files through structural manipulation rather than text-based search and replace. Key semantic entities include `MarkdownParser` class, `mistletoe.Document` AST representation, `mistletoe.block_token.Heading` for header detection, `mistletoe.markdown_renderer.MarkdownRenderer` for output generation, `enhance_tokens_with_blank_lines` spacing helper, `render_with_spacing_preservation` formatting utility, and integration with `...helpers.mistletoe_spacing` module. The parser leverages `mistletoe` line_number attributes for spacing-aware document manipulation preserving original LLM formatting patterns.

##### Main Components

The file contains the `MarkdownParser` class as the primary component providing comprehensive markdown document manipulation capabilities. Core methods include `parse_file` and `parse_content` for document loading, `parse_content_with_spacing_enhancement` for LLM content processing, `find_header_by_text` for header location, `get_section_content` for content extraction, `insert_content_after_header` and `replace_section_content` for content modification, `replace_multiple_sections` for batch operations, `extract_section_content_as_text` for content inspection, and rendering methods `render_to_markdown` and `render_to_markdown_with_spacing`. Supporting methods include `find_available_headers` for document analysis, `analyze_spacing_patterns` for formatting preservation, and `validate_document_structure` for integrity checking.

###### Architecture & Design

The architecture follows AST-based parsing principles using `mistletoe` for reliable markdown structure understanding and manipulation. The design separates parsing logic from content generation through clean interfaces supporting maintainable markdown editing operations. It implements header-based section identification enabling precise content targeting without fragile text matching. The system uses token-level manipulation preserving document structure and formatting during modifications. Error handling provides graceful fallbacks preventing document corruption during parsing failures. The design incorporates spacing-aware rendering using line_number attributes for original formatting preservation. Content manipulation operates at the AST level ensuring valid markdown structure and CommonMark compliance.

####### Implementation Approach

The implementation uses `mistletoe.Document` parsing for complete AST representation enabling structural content manipulation. Header detection traverses document children searching for `Heading` tokens with text content matching. Section boundary detection identifies content scope by finding next header of same or higher level. Content insertion and replacement operate through AST token manipulation maintaining proper parent-child relationships. Spacing preservation analyzes `line_number` attributes calculating blank line patterns from line gaps between consecutive tokens. Batch section updates process multiple replacements maintaining document indexing consistency. Text extraction renders section tokens to markdown for content inspection and validation. The system implements token-by-token rendering with manual spacing insertion for precise formatting control.

######## External Dependencies & Integration Points

**→ Inbound:**
- `mistletoe` (external library) - AST-based markdown parsing and rendering library
- `mistletoe.Document` (external library) - Complete document AST representation
- `mistletoe.block_token.Heading` (external library) - Header token structure for section identification
- `mistletoe.block_token.Paragraph` (external library) - Paragraph token handling for content manipulation
- `mistletoe.block_token.BlockToken` (external library) - Base block token interface
- `mistletoe.span_token.RawText` (external library) - Text content extraction from tokens
- `mistletoe.markdown_renderer.MarkdownRenderer` (external library) - AST to markdown conversion
- `...helpers.mistletoe_spacing:enhance_tokens_with_blank_lines` - Spacing enhancement for LLM content
- `...helpers.mistletoe_spacing:render_with_spacing_preservation` - Spacing-aware rendering utility
- `...helpers.mistletoe_spacing:preserve_llm_spacing` - LLM formatting preservation helper
- `pathlib.Path` (external library) - Cross-platform file operations
- `logging` (external library) - Error reporting and debugging information

**← Outbound:**
- Knowledge base generation systems - Provides markdown parsing and manipulation for content editing
- Template engines - Supplies document structure analysis and section replacement capabilities
- Content validation systems - Offers section extraction and document integrity checking
- Spacing preservation workflows - Delivers formatting-aware rendering for LLM-generated content

**⚡ System role and ecosystem integration:**
- **System Role**: Core markdown processing engine for the Jesse Framework MCP knowledge base system, enabling safe structural editing of existing markdown files without placeholder dependencies
- **Ecosystem Position**: Central to knowledge base maintenance workflows, providing the foundation for content updates, section replacements, and document structure preservation
- **Integration Pattern**: Used by knowledge builders and template generators requiring reliable markdown manipulation, while consuming mistletoe parsing capabilities and spacing preservation helpers for comprehensive document processing

######### Edge Cases & Error Handling

The system handles file reading errors through `FileNotFoundError` catching and graceful None returns enabling fallback processing strategies. Parsing exceptions during `mistletoe.Document` creation are caught with detailed error logging preventing workflow disruption. Header search operations return None when target headers are not found supporting conditional processing logic. Section boundary detection handles documents without proper header hierarchy through empty list returns. Content insertion failures are isolated preventing document corruption through transaction-like error handling. Spacing analysis handles tokens without `line_number` attributes through default spacing fallbacks. Token text extraction handles various token structures including nested formatting through multiple extraction strategies. Document structure validation checks parent-child relationships and header level constraints preventing invalid AST manipulation. Rendering failures fall back to standard markdown output ensuring content is never lost during processing.

########## Internal Implementation Details

The `MarkdownParser` class maintains no persistent state, operating as a stateless service for document manipulation operations. The `_extract_text_from_token` method directly accesses `_children[0].content` for headers containing single `RawText` children, with fallback approaches for complex token structures. Spacing analysis in `analyze_spacing_patterns` creates gap mappings from consecutive token `line_number` differences calculating blank line positions. The `_calculate_appropriate_spacing` method uses token type analysis determining context-aware spacing between headers, paragraphs, and other block elements. Section replacement in `replace_section_content` removes existing tokens in reverse order maintaining proper indexing during AST modification. Multiple section updates process replacements in document order preventing index corruption during batch operations. Document validation checks token parent relationships and header level constraints ensuring AST integrity. Rendering with spacing preservation manually calculates blank lines from line number gaps inserting appropriate newlines between rendered tokens.

########### Code Usage Examples

**Basic document parsing and header detection:**
```python
# Parse markdown file and find specific header for content manipulation
parser = MarkdownParser()
doc = parser.parse_file(Path("knowledge_base.md"))
header = parser.find_header_by_text(doc, "Implementation Details")
```

**Section content replacement with spacing preservation:**
```python
# Replace section content while preserving original LLM formatting patterns
new_content = "## Updated Implementation\nNew content with proper formatting"
updated_doc = parser.replace_section_content(doc, "Implementation Details", new_content)
markdown_output = parser.render_to_markdown_with_spacing(updated_doc)
```

**Batch section updates for multiple content changes:**
```python
# Update multiple sections in single operation maintaining document structure
section_updates = {
    "Architecture Overview": "## New Architecture\nUpdated architectural description",
    "Usage Examples": "## Updated Examples\nNew code examples and patterns"
}
updated_doc = parser.replace_multiple_sections(doc, section_updates)
```

**Content extraction and document analysis:**
```python
# Extract section content for validation and analyze document structure
section_text = parser.extract_section_content_as_text(doc, "Error Handling")
available_headers = parser.find_available_headers(doc)
is_valid = parser.validate_document_structure(doc)
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/plan_generator.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

Decision-to-plan translation engine converting `RebuildDecisionEngine` decisions into comprehensive atomic task execution plans for the Jesse Framework MCP knowledge base system. Provides Plan-then-Execute architecture implementation through `PlanGenerator` class that transforms high-level rebuild and deletion decisions into `ExecutionPlan` objects containing `AtomicTask` instances with proper dependencies and resource estimation. Enables developers to create reliable, debuggable execution workflows with comprehensive task metadata, dependency management, and performance estimation for hierarchical knowledge base processing. Key semantic entities include `PlanGenerator`, `ExecutionPlan`, `AtomicTask`, `TaskType`, `DecisionReport`, `RebuildDecision`, `DeletionDecision`, `DirectoryContext`, `FileContext`, `IndexingConfig`, `DecisionOutcome`, `DecisionReason`, `fastmcp.Context`, `pathlib.Path`, and `datetime` for comprehensive plan generation using five-phase workflow creating cleanup, cache structure, file processing, directory processing, and verification tasks with horizontal dependency management.

##### Main Components

Core `PlanGenerator` class serving as the primary decision-to-plan translation component with comprehensive task generation methods including `create_execution_plan()` for complete plan orchestration, `_generate_cleanup_tasks()` for deletion operations, `_generate_cache_structure_task()` for directory preparation, `_generate_file_tasks()` and `_generate_file_tasks_recursive()` for file processing, and `_generate_directory_tasks()` with `_generate_directory_tasks_recursive()` for knowledge base generation. Supporting utility methods including `_collect_cache_directories()` for structure preparation, `_get_base_dependencies()` for dependency management, `_collect_sibling_directory_tasks()` for horizontal dependencies, and `_sanitize_path_for_id()` for task identifier generation. Performance estimation parameters including `llm_analysis_duration`, `kb_generation_duration`, `file_operation_duration`, and `cleanup_operation_duration` for accurate resource planning.

###### Architecture & Design

Implements five-phase plan generation architecture: Phase 1 generates cleanup tasks with no dependencies for early execution, Phase 2 creates cache structure tasks for directory preparation, Phase 3 generates file processing tasks depending on cleanup and structure, Phase 4 creates directory tasks depending on file tasks with horizontal sibling dependencies, and Phase 5 adds verification tasks depending on all processing operations. Uses leaf-first recursive processing ensuring proper hierarchical dependency ordering where parent directories wait for all child directories and files. Employs atomic task decomposition with comprehensive metadata embedding eliminating external state dependencies during execution. Implements horizontal dependency management through sibling task collection ensuring parent directories synchronize with all sibling completion before processing.

####### Implementation Approach

Executes decision-driven task generation matching `RebuildDecisionEngine` decisions exactly to appropriate `TaskType` instances including `ANALYZE_FILE_LLM`, `SKIP_FILE_CACHED`, `CREATE_DIRECTORY_KB`, `SKIP_DIRECTORY_FRESH`, `DELETE_ORPHANED_FILE`, `DELETE_ORPHANED_DIRECTORY`, `CREATE_CACHE_STRUCTURE`, `VERIFY_CACHE_FRESHNESS`, and `VERIFY_KB_FRESHNESS`. Uses recursive directory traversal with consistent task ID generation through path sanitization ensuring reliable dependency resolution. Implements comprehensive metadata embedding including file characteristics, decision reasoning, directory contexts, and execution parameters for independent task execution. Employs priority-based task ordering with cleanup tasks at priority 100, file tasks at priority 50, directory tasks at priority 30, and verification tasks at priority 10 for optimal execution sequencing.

######## External Dependencies & Integration Points

**→ Inbound:**
- `fastmcp.Context` - FastMCP framework context interface for async progress reporting and logging during plan generation
- `jesse_framework_mcp.knowledge_bases.models:IndexingConfig` - Configuration object providing processing parameters and estimation settings
- `jesse_framework_mcp.knowledge_bases.models:DirectoryContext` - Directory structure representation with file and subdirectory contexts for task creation
- `jesse_framework_mcp.knowledge_bases.models:FileContext` - Individual file metadata including path, size, and timestamps for task metadata embedding
- `jesse_framework_mcp.knowledge_bases.models.rebuild_decisions` - Decision model classes including `DecisionReport`, `RebuildDecision`, `DeletionDecision` for decision-to-task translation
- `jesse_framework_mcp.knowledge_bases.models.execution_plan` - Execution planning models including `ExecutionPlan`, `AtomicTask`, `TaskType` for plan construction
- `pathlib.Path` (external library) - Cross-platform path operations and task target specification
- `datetime` (external library) - Timestamp generation for plan IDs and task metadata
- `logging` (external library) - Structured logging for plan generation analysis and debugging

**← Outbound:**
- `jesse_framework_mcp.knowledge_bases.indexing.hierarchical_indexer:HierarchicalIndexer` - Consumes execution plans for coordinated task execution
- Task execution engines - Receive atomic tasks with complete metadata for independent execution
- Monitoring and progress reporting systems - Consume task estimates and execution planning information
- Debugging and validation tools - Use comprehensive task metadata for execution analysis

**⚡ System role and ecosystem integration:**
- **System Role**: Critical translation component bridging decision-making and execution phases in the Jesse Framework MCP Plan-then-Execute architecture for knowledge base processing
- **Ecosystem Position**: Core component enabling reliable execution planning by converting high-level decisions into atomic, dependency-aware tasks with comprehensive metadata
- **Integration Pattern**: Used by hierarchical indexer to transform decision reports into executable plans, with generated tasks consumed by execution engines and monitoring systems

######### Edge Cases & Error Handling

Handles missing decisions by logging warnings and skipping task creation when `DecisionReport` lacks decisions for specific files or directories. Manages circular dependency prevention by avoiding sibling dependencies in directory task generation, using parent-child relationships exclusively for dependency management. Implements comprehensive plan validation through `ExecutionPlan.validate_dependencies()` ensuring all task dependencies are resolvable before execution begins. Provides graceful error handling in sibling collection with try-catch blocks returning empty lists when path calculations fail. Uses safe path sanitization preventing invalid task IDs from special characters or path separators while maintaining uniqueness across different files and directories.

########## Internal Implementation Details

Maintains performance estimation parameters as instance variables enabling realistic duration calculations for different task types with `llm_analysis_duration` at 30 seconds, `kb_generation_duration` at 15 seconds, and fast operations under 0.1 seconds. Uses consistent task ID generation through `_sanitize_path_for_id()` replacing path separators and special characters with underscores while removing consecutive underscores for clean identifiers. Implements comprehensive metadata embedding including file sizes, modification timestamps, decision reasoning, directory contexts, and dependency counts for complete task execution context. Employs priority-based task ordering ensuring cleanup executes first, followed by structure creation, file processing, directory processing, and verification last through numerical priority values.

########### Code Usage Examples

Essential plan generation workflow for converting decisions to executable tasks:

```python
# Initialize plan generator with configuration for task estimation and metadata
config = IndexingConfig(output_config=OutputConfig(knowledge_output_directory=knowledge_dir))
plan_generator = PlanGenerator(config)

# Create comprehensive execution plan from decision report and directory context
execution_plan = await plan_generator.create_execution_plan(
    root_context, decision_report, source_root, ctx
)

# Access generated tasks and execution metadata for processing coordination
total_tasks = len(execution_plan.tasks)
expensive_tasks = execution_plan.expensive_task_count
estimated_duration = sum(task.estimated_duration for task in execution_plan.tasks)
```

Task dependency analysis and execution ordering for debugging and monitoring:

```python
# Analyze task dependencies and execution ordering for validation
validation_errors = execution_plan.validate_dependencies()
if validation_errors:
    logger.error(f"Plan validation failed: {validation_errors}")

# Access individual tasks with complete metadata for execution engines
for task in execution_plan.tasks:
    task_type = task.task_type
    dependencies = task.dependencies
    metadata = task.metadata
    priority = task.priority
    estimated_duration = task.estimated_duration
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/rebuild_decision_engine.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

Centralized decision engine consolidating all rebuild and deletion decision logic for the Jesse Framework MCP knowledge base hierarchical indexing system. Provides single source of truth for processing decisions eliminating scattered logic across multiple components through `RebuildDecisionEngine` class with comprehensive staleness checking via `FileAnalysisCache` integration. Enables developers to make consistent, auditable decisions about file rebuilds, directory processing, and orphaned content cleanup with rich audit trails and performance optimization. Key semantic entities include `RebuildDecisionEngine`, `DecisionReport`, `RebuildDecision`, `DeletionDecision`, `DecisionOutcome`, `DecisionReason`, `FileAnalysisCache`, `ProjectBaseHandler`, `GitCloneHandler`, `DirectoryContext`, `FileContext`, `IndexingConfig`, `fastmcp.Context`, `pathlib.Path`, and `datetime` for comprehensive decision-making operations using file-first optimization approach preventing unnecessary directory rebuilds when only individual files are stale.

##### Main Components

Core `RebuildDecisionEngine` class serving as the primary decision-making component with comprehensive analysis methods including `analyze_hierarchy()` for full directory tree processing, `_analyze_file_rebuild_decisions()` and `_analyze_directory_rebuild_decisions()` for targeted processing, and `_analyze_deletion_decisions()` for orphaned content cleanup. Supporting decision factory methods `_create_rebuild_decision()` and `_create_deletion_decision()` for standardized decision object creation. Consolidated helper methods including `_get_file_timestamp_safe()`, `_is_timestamp_newer()`, `_calculate_cache_path_safe()`, and path mapping utilities for consistent filesystem operations. Public interface methods `should_rebuild_directory()`, `should_cleanup_orphaned_analysis()`, and `is_cache_fresh()` for external component integration.

###### Architecture & Design

Implements centralized decision architecture consolidating scattered logic from multiple components into single authoritative decision point with comprehensive audit trails. Uses file-first optimization approach analyzing individual file staleness before directory-level decisions to prevent unnecessary rebuilds. Integrates with existing `FileAnalysisCache` for performance-optimized staleness checking and special handlers (`ProjectBaseHandler`, `GitCloneHandler`) for scenario-specific processing. Employs selective cascading pattern where content-driven rebuilds automatically trigger ancestor directory rebuilds through `_propagate_cascading_decisions()`. Implements defensive programming with graceful error handling ensuring decision failures don't break overall processing workflow.

####### Implementation Approach

Executes four-phase analysis workflow: Phase 1 analyzes individual file rebuild decisions using cache staleness checking, Phase 2 analyzes directory rebuild decisions considering file-level outcomes, Phase 3 detects orphaned files for deletion decisions, and Phase 4 implements selective cascading propagating rebuild decisions up hierarchy. Uses timestamp-based staleness detection through direct comparison without tolerance for consistent behavior across components. Implements comprehensive path mapping between source files and knowledge base structure using project-base directory mirroring. Employs consolidated helper methods eliminating DRY violations and providing consistent error handling patterns across all decision operations.

######## External Dependencies & Integration Points

**→ Inbound:**
- `fastmcp.Context` - FastMCP framework context interface for async logging and progress reporting
- `jesse_framework_mcp.knowledge_bases.models:IndexingConfig` - Configuration object providing filtering rules and output directory settings
- `jesse_framework_mcp.knowledge_bases.models:DirectoryContext` - Directory structure representation with file and subdirectory contexts
- `jesse_framework_mcp.knowledge_bases.models:FileContext` - Individual file metadata including path, size, and modification timestamps
- `jesse_framework_mcp.knowledge_bases.models.rebuild_decisions` - Decision model classes for structured outcomes and reasoning
- `jesse_framework_mcp.knowledge_bases.indexing.file_analysis_cache:FileAnalysisCache` - Cache staleness checking and performance optimization
- `jesse_framework_mcp.knowledge_bases.indexing.special_handlers` - Project-base and git-clone scenario handlers
- `pathlib.Path` (external library) - Cross-platform path operations and filesystem metadata access
- `datetime` (external library) - Timestamp comparison and decision timing calculations
- `asyncio` (external library) - Async execution framework for concurrent decision processing

**← Outbound:**
- `jesse_framework_mcp.knowledge_bases.indexing.plan_generator:PlanGenerator` - Consumes decision reports for execution planning
- `jesse_framework_mcp.knowledge_bases.indexing.hierarchical_indexer:HierarchicalIndexer` - Uses rebuild decisions for processing coordination
- Knowledge base processing pipeline - Receives structured decision reports with rebuild and deletion instructions
- Monitoring and logging systems - Consumes performance statistics and decision audit trails

**⚡ System role and ecosystem integration:**
- **System Role**: Core decision-making component serving as single source of truth for all rebuild and deletion decisions within the Jesse Framework MCP knowledge base indexing system
- **Ecosystem Position**: Central component eliminating scattered decision logic across multiple indexing components while providing comprehensive audit trails and performance optimization
- **Integration Pattern**: Used by hierarchical indexer and plan generator for coordinated processing decisions, with results feeding into execution planning and monitoring systems

######### Edge Cases & Error Handling

Handles empty directories by detecting contentless directories and skipping knowledge file generation to prevent infinite rebuild loops through `_is_directory_empty_of_processable_content()`. Manages project root specially ensuring `root_kb.md` generation follows business rules while applying standard staleness checking. Implements graceful filesystem error handling with conservative fallback decisions when file access fails or path calculations encounter errors. Provides comprehensive orphaned file detection with safety validation preventing accidental deletion of valid content. Uses consolidated error handling patterns through `_handle_decision_error()` creating standardized error decisions with detailed context information for debugging and recovery.

########## Internal Implementation Details

Maintains performance tracking through `_decisions_made`, `_filesystem_operations`, and `_decision_start_time` counters for optimization monitoring. Uses cached `_project_base_root` path for performance optimization in repeated path calculations. Implements consolidated helper methods eliminating DRY violations including timestamp retrieval, path calculation, and decision creation patterns. Employs direct timestamp comparison without tolerance aligning with `FileAnalysisCache` behavior for consistent staleness detection. Tracks cascading decisions through ancestor path calculation using `_get_ancestor_directories()` with safety limits preventing infinite traversal loops.

########### Code Usage Examples

Essential decision engine initialization and hierarchy analysis pattern:

```python
# Initialize decision engine with configuration for centralized decision making
config = IndexingConfig(output_config=OutputConfig(knowledge_output_directory=knowledge_dir))
decision_engine = RebuildDecisionEngine(config)

# Perform comprehensive hierarchy analysis with file-first optimization
report = await decision_engine.analyze_hierarchy(root_context, source_root, ctx)

# Access structured decision results for execution planning
stats = report.get_summary_statistics()
rebuild_decisions = report.rebuild_decisions
deletion_decisions = report.deletion_decisions
```

Individual decision making for external component integration:

```python
# Make single directory rebuild decision for targeted processing
directory_decision = await decision_engine.should_rebuild_directory(
    directory_context, source_root, ctx
)

# Check cache freshness for individual file processing optimization
is_fresh, reason = decision_engine.is_cache_fresh(file_path, source_root)

# Determine orphaned file cleanup status for maintenance operations
cleanup_outcome, cleanup_reason = await decision_engine.should_cleanup_orphaned_analysis(
    analysis_file_path, source_root, ctx
)
```

### {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing/special_handlers.py

*Last Updated: 2025-07-06T23:05:36Z*

#### Functional Intent & Features

This file implements specialized handlers for the Jesse Framework MCP Knowledge Bases Hierarchical Indexing System, providing two distinct processing strategies for unique scenarios. The `GitCloneHandler` enables read-only processing of git clones with mirrored knowledge structure preservation, while the `ProjectBaseHandler` performs whole codebase indexing with systematic exclusion rules. Key semantic entities include `GitCloneHandler` and `ProjectBaseHandler` classes, `IndexingConfig` and `DirectoryContext` models, `pathlib.Path` operations, and specialized methods like `get_mirrored_knowledge_path()` and `should_process_project_item()`. The handlers integrate with the core hierarchical indexing system while maintaining specialized processing logic for git-clones (read-only mirrored structure) and project-base (comprehensive codebase coverage) scenarios, utilizing defensive programming patterns to handle access restrictions and filesystem errors gracefully.

##### Main Components

The file contains two primary handler classes: `GitCloneHandler` for processing read-only git clone repositories with mirrored knowledge structure creation, and `ProjectBaseHandler` for comprehensive project codebase indexing with exclusion filtering. Each handler includes initialization methods accepting `IndexingConfig`, path detection methods (`is_git_clone_path()`, `should_process_project_item()`), specialized path mapping functions (`get_mirrored_knowledge_path()`, `get_project_knowledge_path()`), and comprehensive structure processing methods (`process_git_clone_structure()`, `process_project_structure()`). The `ProjectBaseHandler` additionally implements `_build_project_directory_context()` for recursive directory traversal and context building with project-specific filtering rules.

###### Architecture & Design

Both handlers follow a configuration-driven architecture pattern with dependency injection of `IndexingConfig` objects, enabling flexible behavior customization. The `GitCloneHandler` implements a mirrored structure pattern, mapping git clone paths to parallel knowledge base directories with `_kb` suffixes while preserving directory hierarchy relationships. The `ProjectBaseHandler` employs a comprehensive traversal pattern with systematic exclusion rules, maintaining a predefined set of system exclusions (`.git`, `.knowledge`, `.coding_assistant`, `__pycache__`, etc.) and integrating with configuration-based filtering. Both handlers utilize defensive programming patterns with comprehensive error handling and logging, ensuring graceful degradation when encountering access restrictions or filesystem errors.

####### Implementation Approach

The `GitCloneHandler` uses path manipulation algorithms to create mirrored knowledge structures, converting paths like `.knowledge/git-clones/repo/file.py` to `.knowledge/git-clones/repo_kb/file_kb.md` while preserving directory hierarchies. The `ProjectBaseHandler` implements recursive directory traversal with filtering at each level, using `iterdir()` for filesystem enumeration and applying exclusion rules through `should_process_project_item()`. Both handlers create `DirectoryContext` and `FileContext` objects with metadata extraction including file sizes and modification timestamps. The implementation uses async/await patterns for progress reporting through `Context` objects, enabling user feedback during large codebase processing operations with structured logging for debugging and monitoring.

######## External Dependencies & Integration Points

**→ Inbound:**
- `..models.indexing_config:IndexingConfig` - configuration and exclusion rules for specialized processing
- `..models.knowledge_context:DirectoryContext` - context structures for directory representation
- `..models.knowledge_context:FileContext` - context structures for file representation
- `pathlib.Path` (standard library) - cross-platform path operations and directory traversal
- `logging` (standard library) - structured logging for special handling operations
- `fastmcp.Context` (external library) - MCP context for async progress reporting

**← Outbound:**
- `core/hierarchical_processor.py` - consumes specialized handler outputs for knowledge base generation
- `.knowledge/git-clones/{repo}_kb/` - generates mirrored knowledge structure directories
- `.knowledge/project-base/root_kb.md` - produces project root knowledge files

**⚡ System role and ecosystem integration:**
- **System Role**: Specialized processing layer bridging unique scenarios (git clones, project-base) with core hierarchical indexing workflow
- **Ecosystem Position**: Core component enabling comprehensive knowledge base coverage for diverse source structures
- **Integration Pattern**: Used by hierarchical indexing orchestrator to handle special cases requiring custom processing logic, with handlers selected based on path patterns and processing requirements

######### Edge Cases & Error Handling

Both handlers implement comprehensive error handling for filesystem access restrictions, with `try-except` blocks around directory iteration and file access operations. The `GitCloneHandler` handles path generation failures by returning fallback paths (`unknown_kb.md`) and logs errors for debugging. The `ProjectBaseHandler` gracefully handles `OSError` and `PermissionError` exceptions during directory traversal, continuing processing when individual items are inaccessible. Both handlers use warning-level logging for non-critical failures and error-level logging for processing failures that affect overall operation. The handlers include defensive checks for empty path components and invalid directory structures, ensuring robust operation across diverse filesystem configurations and permission scenarios.

########## Internal Implementation Details

The `GitCloneHandler` maintains internal path mapping logic using `relative_to()` and path part manipulation to construct mirrored structures, with special handling for repository root detection and file extension removal. The `ProjectBaseHandler` maintains a `system_exclusions` set as a class attribute, initialized with common development environment directories and cache folders. Both handlers use `datetime.fromtimestamp()` for file modification time extraction and `stat()` for file size metadata. The recursive `_build_project_directory_context()` method implements depth-first traversal with immediate filtering application, creating `FileContext` and `DirectoryContext` objects with complete metadata population. Internal logging uses structured messages with path information for debugging and monitoring specialized processing operations.

########### Code Usage Examples

**GitCloneHandler initialization and path mapping:**
```python
# Initialize handler with configuration
config = IndexingConfig()
git_handler = GitCloneHandler(config)

# Check if path requires git clone handling
if git_handler.is_git_clone_path(Path(".knowledge/git-clones/my-repo")):
    # Generate mirrored knowledge path
    knowledge_path = git_handler.get_mirrored_knowledge_path(
        Path(".knowledge/git-clones/my-repo/src/main.py"),
        Path(".knowledge")
    )
    # Result: .knowledge/git-clones/my-repo_kb/src/main_kb.md
```

**ProjectBaseHandler filtering and processing:**
```python
# Initialize handler with exclusion rules
project_handler = ProjectBaseHandler(config)

# Check if project item should be processed
should_process = project_handler.should_process_project_item(
    Path("src/main.py"), 
    Path(".")
)

# Get project knowledge file location
kb_path = project_handler.get_project_knowledge_path(Path("."))
# Result: ./.knowledge/project-base/root_kb.md
```

---
*Generated: 2025-07-06T23:05:36Z*
*Source Directory: {PROJECT_ROOT}/jesse-framework-mcp/jesse_framework_mcp/knowledge_bases/indexing*
*Total Files: 14*
*Total Subdirectories: 1*

# End of indexing_kb.md